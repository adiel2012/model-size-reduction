{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Knowledge Distillation From Scratch: Teacher to Student\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/distillation_demo.ipynb)\n",
    "\n",
    "## ðŸ“– The Theory: Transferring dark knowledge\n",
    "\n",
    "Knowledge Distillation (KD) is a compression method where a compact model (**Student**) learns to mimic a large model (**Teacher**). Hinton et al. (2015) introduced the concept of \"Dark Knowledge\"â€”the relative probabilities of incorrect classes assigned by the teacher, which reveal the teacher's internal structure.\n",
    "\n",
    "### The Loss Function\n",
    "The student is trained to minimize a weighted sum of two losses:\n",
    "1.  **Distillation Loss**: KL-Divergence between the teacher's and student's soft-targets.\n",
    "2.  **Student Loss**: Standard Cross-Entropy between the student's output and the ground truth (hard labels).\n",
    "\n",
    "### Temperature (T)\n",
    "We use a temperature $T$ to soften the probability distributions. A higher $T$ makes the \"dark knowledge\" (the small probabilities of incorrect classes) more visible.\n",
    "\n",
    "$$q_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n",
    "\n",
    "The distillation loss is then scaled by $T^2$ to keep the gradient magnitudes consistent when changing $T$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "def manual_distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Implementation of the Distillation Loss from scratch.\n",
    "    \"\"\"\n",
    "    # 1. Soft Targets (Teacher and Student)\n",
    "    # We use log_softmax for the student and softmax for the teacher for KLDivLoss\n",
    "    soft_targets = F.softmax(teacher_logits / T, dim=-1)\n",
    "    soft_prob = F.log_softmax(student_logits / T, dim=-1)\n",
    "    \n",
    "    distillation_loss = nn.KLDivLoss(reduction=\"batchmean\")(soft_prob, soft_targets) * (T * T)\n",
    "    \n",
    "    # 2. Standard Cross Entropy\n",
    "    # student_logits: [batch, seq, vocab] -> [batch*seq, vocab]\n",
    "    # labels: [batch, seq]\n",
    "    student_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    # 3. Combined Loss\n",
    "    return alpha * distillation_loss + (1 - alpha) * student_loss\n",
    "\n",
    "# 1. Setup Models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "teacher = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval()\n",
    "\n",
    "# Student: 2 layers, small hidden size\n",
    "student_cfg = GPT2Config(n_layer=2, n_head=4, n_embd=256, vocab_size=tokenizer.vocab_size)\n",
    "student = GPT2LMHeadModel(student_cfg)\n",
    "\n",
    "print(f\"Teacher Size: {sum(p.numel() for p in teacher.parameters())/1e6:.1f}M\")\n",
    "print(f\"Student Size: {sum(p.numel() for p in student.parameters())/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ”¢ Worked Example with numbers\n\nA step-by-step trace of `manual_distillation_loss` with a tiny input so you can verify every value by hand.\n\n- Vocabulary of **3 tokens**, batch size **1**, sequence length **1**\n- Temperature **T = 2.0**, mixing weight **Î± = 0.5**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch, torch.nn as nn, torch.nn.functional as F\n\nT, alpha = 2.0, 0.5\n\n# Tiny inputs: batch=1, seq=1, vocab=3\nteacher_logits = torch.tensor([[[2.0, 1.0, 0.0]]])   # shape [1, 1, 3]\nstudent_logits = torch.tensor([[[1.5, 0.5, 0.0]]])   # shape [1, 1, 3]\nlabels         = torch.tensor([[0]])                  # correct token = index 0\n\n# â”€â”€ Step 1 : Soft targets (Teacher) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# teacher_logits / T  =>  [1.0000,  0.5000,  0.0000]\n# exp(...)            =>  [2.7183,  1.6487,  1.0000]   sum = 5.3670\nsoft_targets = F.softmax(teacher_logits / T, dim=-1)\n# soft_targets        =>  [0.5065,  0.3072,  0.1863]\n\n# â”€â”€ Step 2 : Soft log-probabilities (Student) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# student_logits / T  =>  [0.7500,  0.2500,  0.0000]\n# exp(...)            =>  [2.1170,  1.2840,  1.0000]   sum = 4.4010\nsoft_prob = F.log_softmax(student_logits / T, dim=-1)\n# soft_prob           =>  [-0.7315, -1.2315, -1.4815]\n\n# â”€â”€ Step 3 : KL-Divergence loss (scaled by TÂ²) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# KLDiv = Î£ soft_targets Ã— (log(soft_targets) âˆ’ soft_prob)\n#       = 0.5065Ã—0.0504 + 0.3072Ã—0.0504 + 0.1863Ã—(âˆ’0.1996)  â‰ˆ  0.0038\n# Ã— TÂ²  = 0.0038 Ã— 4                                         â‰ˆ  0.0153\ndistillation_loss = nn.KLDivLoss(reduction=\"batchmean\")(soft_prob, soft_targets) * (T * T)\n# distillation_loss   =>  â‰ˆ 0.0153\n\n# â”€â”€ Step 4 : Cross-Entropy loss (Student vs hard label) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# student_logits flat  =>  [[1.5, 0.5, 0.0]],  label = [0]\n# exp(...)             =>  [4.4817, 1.6487, 1.0000]   sum = 7.1304\n# P(class=0)          =   4.4817 / 7.1304  â‰ˆ  0.6285\n# CE = âˆ’log(0.6285)                         â‰ˆ  0.4644\nstudent_loss = F.cross_entropy(\n    student_logits.view(-1, student_logits.size(-1)),  # => shape [1, 3]\n    labels.view(-1)                                    # => shape [1]\n)\n# student_loss        =>  â‰ˆ 0.4644\n\n# â”€â”€ Step 5 : Combined loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# = 0.5 Ã— 0.0153 + 0.5 Ã— 0.4644\n# = 0.0077         + 0.2322         â‰ˆ  0.2399\ncombined_loss = alpha * distillation_loss + (1 - alpha) * student_loss\n# combined_loss       =>  â‰ˆ 0.2399\n\nprint(f\"soft_targets      : {[round(v,4) for v in soft_targets.squeeze().tolist()]}\")\n# => [0.5065, 0.3072, 0.1863]\nprint(f\"soft_prob         : {[round(v,4) for v in soft_prob.squeeze().tolist()]}\")\n# => [-0.7315, -1.2315, -1.4815]\nprint(f\"distillation_loss : {distillation_loss.item():.4f}\")   # => 0.0153\nprint(f\"student_loss      : {student_loss.item():.4f}\")        # => 0.4644\nprint(f\"combined_loss     : {combined_loss.item():.4f}\")       # => 0.2399",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ The Distillation Loop\n",
    "A manual step-by-step distillation training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
    "\n",
    "inputs = tokenizer(\"The quick brown fox jumps over the lazy dog\", return_tensors=\"pt\")\n",
    "labels = inputs[\"input_ids\"]\n",
    "\n",
    "# Step 1: Get Teacher Knowledge (no gradients here)\n",
    "with torch.no_grad():\n",
    "    teacher_out = teacher(**inputs)\n",
    "    teacher_logits = teacher_out.logits\n",
    "\n",
    "# Step 2: Student Forward Pass\n",
    "student_out = student(**inputs)\n",
    "student_logits = student_out.logits\n",
    "\n",
    "# Step 3: Compute Loss & Update\n",
    "loss = manual_distillation_loss(student_logits, teacher_logits, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Training step lost: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}