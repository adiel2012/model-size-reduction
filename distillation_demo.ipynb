{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ“ Knowledge Distillation: From Teacher to Student\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-quantization/blob/main/distillation_demo.ipynb)\n",
                "\n",
                "Knowledge Distillation (KD) is a model compression technique where a smaller model (**Student**) is trained to reproduce the behavior of a larger, pre-trained model (**Teacher**). \n",
                "\n",
                "Instead of just training on hard labels (correct/incorrect), the student learns from the teacher's \"soft targets\"â€”the probability distributions over all classes. These soft targets contain rich information about the teacher's internal logic."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers datasets torch -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
                "\n",
                "# 1. Load Teacher (GPT-2)\n",
                "teacher_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "teacher_model.eval() # Freeze teacher\n",
                "\n",
                "# 2. Initialize a much smaller Student\n",
                "# GPT-2 has 12 layers. Let's make a 2-layer student.\n",
                "student_config = GPT2Config(\n",
                "    n_layer=2, \n",
                "    n_head=8, \n",
                "    n_embd=512, # smaller embedding\n",
                "    vocab_size=tokenizer.vocab_size\n",
                ")\n",
                "student_model = GPT2LMHeadModel(student_config)\n",
                "\n",
                "print(f\"Teacher Parameters: {sum(p.numel() for p in teacher_model.parameters())/1e6:.1f}M\")\n",
                "print(f\"Student Parameters: {sum(p.numel() for p in student_model.parameters())/1e6:.1f}M\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  The Distillation Loss\n",
                "We use **KL-Divergence** to measure how well the student's probability distribution matches the teacher's. We also use a **Temperature** (T) parameter to smooth the distributions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
                "    # 1. Distillation Loss (KL Divergence)\n",
                "    # Filter out labels to match teacher_logits if needed, but for LM we usually match the whole sequence\n",
                "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
                "        F.log_softmax(student_logits / T, dim=-1),\n",
                "        F.softmax(teacher_logits / T, dim=-1)\n",
                "    ) * (T * T)\n",
                "    \n",
                "    # 2. Hard Label Loss (Standard Cross Entropy)\n",
                "    hard_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
                "    \n",
                "    return alpha * soft_loss + (1 - alpha) * hard_loss\n",
                "\n",
                "print(\"Distillation loss function defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”„ Training Loop Mockup\n",
                "In a real scenario, you would run this over a large dataset (like WikiText or OpenWebText)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizer = torch.optim.AdamW(student_model.parameters(), lr=5e-5)\n",
                "\n",
                "dummy_input = tokenizer(\"Knowledge distillation is a powerful technique\", return_tensors=\"pt\")\n",
                "labels = dummy_input[\"input_ids\"]\n",
                "\n",
                "# Forward pass\n",
                "with torch.no_grad():\n",
                "    teacher_outputs = teacher_model(**dummy_input)\n",
                "    teacher_logits = teacher_outputs.logits\n",
                "\n",
                "student_outputs = student_model(**dummy_input)\n",
                "student_logits = student_outputs.logits\n",
                "\n",
                "loss = distillation_loss(student_logits, teacher_logits, labels)\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(f\"Distillation Step Complete. Loss: {loss.item():.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}