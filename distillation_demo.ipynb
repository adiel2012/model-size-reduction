{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ“ Knowledge Distillation From Scratch: Teacher to Student\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/distillation_demo.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: Transferring dark knowledge\n",
                "\n",
                "Knowledge Distillation (KD) is a compression method where a compact model (**Student**) learns to mimic a large model (**Teacher**). Hinton et al. (2015) introduced the concept of \"Dark Knowledge\"â€”the relative probabilities of incorrect classes assigned by the teacher, which reveal the teacher's internal structure.\n",
                "\n",
                "### The Loss Function\n",
                "The student is trained to minimize a weighted sum of two losses:\n",
                "1.  **Distillation Loss**: KL-Divergence between the teacher's and student's soft-targets.\n",
                "2.  **Student Loss**: Standard Cross-Entropy between the student's output and the ground truth (hard labels).\n",
                "\n",
                "### Temperature (T)\n",
                "We use a temperature $T$ to soften the probability distributions. A higher $T$ makes the \"dark knowledge\" (the small probabilities of incorrect classes) more visible.\n",
                "\n",
                "$$q_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n",
                "\n",
                "The distillation loss is then scaled by $T^2$ to keep the gradient magnitudes consistent when changing $T$.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
                "\n",
                "def manual_distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
                "    \"\"\"\n",
                "    Implementation of the Distillation Loss from scratch.\n",
                "    \"\"\"\n",
                "    # 1. Soft Targets (Teacher and Student)\n",
                "    # We use log_softmax for the student and softmax for the teacher for KLDivLoss\n",
                "    soft_targets = F.softmax(teacher_logits / T, dim=-1)\n",
                "    soft_prob = F.log_softmax(student_logits / T, dim=-1)\n",
                "    \n",
                "    distillation_loss = nn.KLDivLoss(reduction=\"batchmean\")(soft_prob, soft_targets) * (T * T)\n",
                "    \n",
                "    # 2. Standard Cross Entropy\n",
                "    # student_logits: [batch, seq, vocab] -> [batch*seq, vocab]\n",
                "    # labels: [batch, seq]\n",
                "    student_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
                "    \n",
                "    # 3. Combined Loss\n",
                "    return alpha * distillation_loss + (1 - alpha) * student_loss\n",
                "\n",
                "# 1. Setup Models\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "teacher = GPT2LMHeadModel.from_pretrained(\"gpt2\").eval()\n",
                "\n",
                "# Student: 2 layers, small hidden size\n",
                "student_cfg = GPT2Config(n_layer=2, n_head=4, n_embd=256, vocab_size=tokenizer.vocab_size)\n",
                "student = GPT2LMHeadModel(student_cfg)\n",
                "\n",
                "print(f\"Teacher Size: {sum(p.numel() for p in teacher.parameters())/1e6:.1f}M\")\n",
                "print(f\"Student Size: {sum(p.numel() for p in student.parameters())/1e6:.1f}M\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”„ The Distillation Loop\n",
                "A manual step-by-step distillation training step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n",
                "\n",
                "inputs = tokenizer(\"The quick brown fox jumps over the lazy dog\", return_tensors=\"pt\")\n",
                "labels = inputs[\"input_ids\"]\n",
                "\n",
                "# Step 1: Get Teacher Knowledge (no gradients here)\n",
                "with torch.no_grad():\n",
                "    teacher_out = teacher(**inputs)\n",
                "    teacher_logits = teacher_out.logits\n",
                "\n",
                "# Step 2: Student Forward Pass\n",
                "student_out = student(**inputs)\n",
                "student_logits = student_out.logits\n",
                "\n",
                "# Step 3: Compute Loss & Update\n",
                "loss = manual_distillation_loss(student_logits, teacher_logits, labels)\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(f\"Training step lost: {loss.item():.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}