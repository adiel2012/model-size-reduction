{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form"
            },
            "source": [
                "# @title 1. Universal Setup (Run All Compatible)\n",
                "# @markdown This cell installs dependencies and handles Google Colab environment fixes.\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "\n",
                "def setup_colab():\n",
                "    # 1. Check if we are in Colab\n",
                "    try:\n",
                "        import google.colab\n",
                "        IN_COLAB = True\n",
                "    except ImportError:\n",
                "        IN_COLAB = False\n",
                "\n",
                "    if IN_COLAB:\n",
                "        print(\"üåê Running in Google Colab. Checking environment...\")\n",
                "        \n",
                "        # 2. Check for NumPy 2.x (Mandatory for modern Colab TF/JAX builds)\n",
                "        import numpy\n",
                "        if int(numpy.__version__.split('.')[0]) < 2:\n",
                "            print(\"‚è´ Upgrading NumPy to 2.x to fix binary incompatibility...\")\n",
                "            !pip install -q --upgrade \"numpy>=2.0\" tensorflow-model-optimization pandas matplotlib tabulate\n",
                "            print(\"\\n‚ö†Ô∏è RESTARTING RUNTIME: NumPy upgrade requires a session reset.\")\n",
                "            print(\"Execution will stop. Please click 'Run All' again after the restart is complete.\")\n",
                "            os.kill(os.getpid(), 9)\n",
                "        \n",
                "        # 3. Check for specific libraries if NumPy is already fine\n",
                "        try:\n",
                "            import tensorflow_model_optimization\n",
                "        except ImportError:\n",
                "            print(\"üì¶ Installing missing experiment libraries...\")\n",
                "            !pip install -q tensorflow-model-optimization pandas matplotlib tabulate\n",
                "\n",
                "    # 4. Final settings\n",
                "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
                "    warnings.filterwarnings('ignore', category=UserWarning)\n",
                "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
                "    print(\"‚úÖ Environment Ready!\")\n",
                "\n",
                "setup_colab()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ The Ultimate Quantization Benchmark: Research to Production\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/experiment_framework.ipynb)\n",
                "\n",
                "## üìñ Overview\n",
                "This notebook provides a unified experimentation framework to compare the major quantization milestones from 2022 to 2026. While the chronology folders contain \"from scratch\" implementations for learning, this framework uses **TensorFlow (TFLite & TFMOT)** built-in functions to simulate these algorithms in a production-ready environment.\n",
                "\n",
                "### Algorithms Compared\n",
                "1.  **Baseline (FP32)**: The uncompressed reference model.\n",
                "2.  **LLM.int8() style**: Dynamic Range Quantization (Weight INT8).\n",
                "3.  **GPTQ / AWQ style**: Full Integer Quantization (Calibrated INT8).\n",
                "4.  **NF4 / HQQ style**: 4-bit Weight-only Quantization.\n",
                "5.  **BitNet / T-Poti style**: Simulated ultra-low precision (Sparsity + Quantization).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import time\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow_model_optimization as tfmot\n",
                "\n",
                "print(\"üöÄ TensorFlow version:\", tf.__version__)\n",
                "device_name = tf.test.gpu_device_name()\n",
                "if device_name != '/device:GPU:0':\n",
                "  print('‚ö†Ô∏è GPU not found! Benchmarking on CPU will be slower.')\n",
                "else:\n",
                "  print('‚úÖ Found GPU at: {}'.format(device_name))\n",
                "\n",
                "# 1. Setup Benchmark Model (MNIST CNN)\n",
                "def create_benchmark_model():\n",
                "    model = tf.keras.Sequential([\n",
                "        tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
                "        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
                "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
                "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
                "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
                "        tf.keras.layers.Flatten(),\n",
                "        tf.keras.layers.Dense(10)\n",
                "    ])\n",
                "    return model\n",
                "\n",
                "base_model = create_benchmark_model()\n",
                "base_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
                "\n",
                "# Load data for calibration\n",
                "mnist = tf.keras.datasets.mnist\n",
                "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
                "train_images = train_images.astype(np.float32) / 255.0\n",
                "test_images = test_images.astype(np.float32) / 255.0\n",
                "\n",
                "def representative_data_gen():\n",
                "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
                "        yield [input_value]\n",
                "\n",
                "print(\"‚úÖ Benchmark environment ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Running the Experiment\n",
                "We will now programmatically convert the model using different strategies and measure the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "results = []\n",
                "\n",
                "def run_benchmark(model_content, name):\n",
                "    file_name = f\"{name}.tflite\"\n",
                "    with open(file_name, \"wb\") as f: f.write(model_content)\n",
                "    size_kb = os.path.getsize(file_name) / 1024\n",
                "    \n",
                "    interpreter = tf.lite.Interpreter(model_content=model_content)\n",
                "    interpreter.allocate_tensors()\n",
                "    input_details = interpreter.get_input_details()\n",
                "    output_details = interpreter.get_output_details()\n",
                "    input_idx = input_details[0]['index']\n",
                "    output_idx = output_details[0]['index']\n",
                "    \n",
                "    # Warmup\n",
                "    interpreter.set_tensor(input_idx, test_images[0:1])\n",
                "    interpreter.invoke()\n",
                "    \n",
                "    # Inference Latency (ms)\n",
                "    start = time.time()\n",
                "    for _ in range(200):\n",
                "        interpreter.set_tensor(input_idx, test_images[0:1])\n",
                "        interpreter.invoke()\n",
                "    latency_ms = (time.time() - start) * 5.0 # (Total/200)*1000 = Total*5\n",
                "    \n",
                "    # Accuracy Measurement (on a subset of test images)\n",
                "    correct = 0\n",
                "    total = 500\n",
                "    \n",
                "    # Handle quantization scale if input is int8\n",
                "    if input_details[0]['dtype'] == np.int8:\n",
                "        scale, zero_point = input_details[0]['quantization']\n",
                "        test_images_q = np.array(test_images[:total] / scale + zero_point, dtype=np.int8)\n",
                "    else:\n",
                "        test_images_q = test_images[:total]\n",
                "\n",
                "    for i in range(total):\n",
                "        interpreter.set_tensor(input_idx, test_images_q[i:i+1])\n",
                "        interpreter.invoke()\n",
                "        output = interpreter.get_tensor(output_idx)\n",
                "        prediction = np.argmax(output)\n",
                "        if prediction == test_labels[i]:\n",
                "            correct += 1\n",
                "    \n",
                "    accuracy = (correct / total) * 100\n",
                "    \n",
                "    return {\"Algorithm\": name, \"Size (KB)\": size_kb, \"Latency (ms)\": latency_ms, \"Accuracy (%)\": accuracy}\n",
                "\n",
                "print(\"üöÄ Starting full experimentation sweep...\")\n",
                "\n",
                "# 1. Baseline FP32\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "results.append(run_benchmark(conv.convert(), \"Baseline_FP32\"))\n",
                "\n",
                "# 2. [2022] LLM.int8 style (Dynamic Range)\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "results.append(run_benchmark(conv.convert(), \"LLM_int8_Dynamic\"))\n",
                "\n",
                "# 3. [2023] GPTQ / AWQ style (Full Integer)\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "conv.representative_dataset = representative_data_gen\n",
                "conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
                "conv.inference_input_type = tf.int8\n",
                "conv.inference_output_type = tf.int8\n",
                "results.append(run_benchmark(conv.convert(), \"GPTQ_AWQ_FullInt\"))\n",
                "\n",
                "# 4. [2024] NF4 / HQQ style (4-bit experimental)\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "conv._experimental_new_quantizer = True\n",
                "results.append(run_benchmark(conv.convert(), \"NF4_HQQ_4bit\"))\n",
                "\n",
                "# 5. [2025/2026] BitNet / T-Poti style (Extreme Sparsity + Quantization)\n",
                "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
                "pruned_model = prune_low_magnitude(base_model, tfmot.sparsity.keras.ConstantSparsity(0.5, 0))\n",
                "pruned_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT, tf.lite.Optimize.EXPERIMENTAL_SPARSITY]\n",
                "results.append(run_benchmark(conv.convert(), \"BitNet_TPoti_Extreme\"))\n",
                "\n",
                "print(\"‚úÖ Experiment complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Analysis & Comparison\n",
                "The results below show the clear trade-offs between algorithm sophistication, model size, inference speed, and **predictive accuracy**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "df = pd.DataFrame(results)\n",
                "print(\"\\n--- Final Comparison Table ---\")\n",
                "print(df.to_markdown(index=False))\n",
                "\n",
                "# Visualization\n",
                "fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(12, 10))\n",
                "\n",
                "# Plot 1: Size vs Latency\n",
                "color = 'tab:red'\n",
                "ax1.set_xlabel('Algorithm')\n",
                "ax1.set_ylabel('Size (KB)', color=color)\n",
                "ax1.bar(df['Algorithm'], df['Size (KB)'], color=color, alpha=0.3, label='Model Size')\n",
                "ax1.tick_params(axis='y', labelcolor=color)\n",
                "ax1.set_xticks(range(len(df['Algorithm'])))\n",
                "ax1.set_xticklabels(df['Algorithm'], rotation=30)\n",
                "\n",
                "ax2 = ax1.twinx()\n",
                "color = 'tab:blue'\n",
                "ax2.set_ylabel('Latency (ms)', color=color)\n",
                "ax2.plot(df['Algorithm'], df['Latency (ms)'], color=color, marker='o', linewidth=2, markersize=8, label='Latency')\n",
                "ax2.tick_params(axis='y', labelcolor=color)\n",
                "ax1.set_title('Size and Latency Comparison')\n",
                "\n",
                "# Plot 2: Accuracy\n",
                "color = 'tab:green'\n",
                "ax3.set_xlabel('Algorithm')\n",
                "ax3.set_ylabel('Accuracy (%)', color=color)\n",
                "ax3.bar(df['Algorithm'], df['Accuracy (%)'], color=color, alpha=0.5, label='Accuracy')\n",
                "ax3.tick_params(axis='y', labelcolor=color)\n",
                "ax3.set_ylim(min(df['Accuracy (%)']) - 2, 100)\n",
                "ax3.set_xticks(range(len(df['Algorithm'])))\n",
                "ax3.set_xticklabels(df['Algorithm'], rotation=30)\n",
                "ax3.set_title('Accuracy Comparison')\n",
                "\n",
                "fig.tight_layout()\n",
                "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}