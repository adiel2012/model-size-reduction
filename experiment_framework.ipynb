{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª Experimentation Framework From Scratch\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/experiment_framework.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: Evaluation Metrics for Compression\n",
                "\n",
                "When compressing models, we track the **Pareto Frontier** between quality and efficiency. \n",
                "\n",
                "### 1. Throughput (Tokens/sec)\n",
                "Measures how many tokens the model generates per second. This is the primary metric for user experience in real-time LLM apps.\n",
                "$$\\text{Throughput} = \\frac{N_{\\text{tokens}}}{\\Delta t}$$\n",
                "\n",
                "### 2. Compression Ratio\n",
                "The ratio of original size to compressed size.\n",
                "$$\\text{Ratio} = \\frac{\\text{Size}_{FP32}}{\\text{Size}_{compressed}}$$\n",
                "\n",
                "### 3. Degradation (Perplexity Delta)\n",
                "Measures how much accuracy or coherence is lost. For LLMs, we often use **Perplexity** or **Zero-shot Acc** on benchmarks like MMLU.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import time\n",
                "import pandas as pd\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "\n",
                "class CompressionBenchmark:\n",
                "    \"\"\"\n",
                "    A unified class to measure and compare different models.\n",
                "    \"\"\"\n",
                "    def __init__(self, model_name=\"gpt2\"):\n",
                "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
                "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "    def run(self, model, name):\n",
                "        model.to(self.device).eval()\n",
                "        \n",
                "        # 1. Size Calculation (Manual parameter counting x bytes)\n",
                "        param_count = sum(p.numel() for p in model.parameters())\n",
                "        # Simple heuristic for size in MB\n",
                "        size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)\n",
                "        \n",
                "        # 2. Latency / Throughput\n",
                "        prompt = \"The key to model optimization is\"\n",
                "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
                "        \n",
                "        # Warmup\n",
                "        _ = model.generate(**inputs, max_new_tokens=5)\n",
                "        \n",
                "        start = time.time()\n",
                "        with torch.no_grad():\n",
                "            # Generate fixed number of tokens\n",
                "            _ = model.generate(**inputs, max_new_tokens=30, min_new_tokens=30)\n",
                "        duration = time.time() - start\n",
                "        \n",
                "        throughput = 30 / duration\n",
                "        \n",
                "        return {\n",
                "            \"Method\": name,\n",
                "            \"Size (MB)\": f\"{size_mb:.2f}\",\n",
                "            \"Throughput (tok/s)\": f\"{throughput:.2f}\",\n",
                "            \"Latency (s)\": f\"{duration:.4f}\"\n",
                "        }\n",
                "\n",
                "benchmark = CompressionBenchmark()\n",
                "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
                "results = benchmark.run(base_model, \"Baseline (FP32)\")\n",
                "print(results)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}