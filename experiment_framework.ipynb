{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ Unified Model Compression Experimentation Framework\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-quantization/blob/main/experiment_framework.ipynb)\n",
                "\n",
                "This notebook provides a modular framework to compare different model compression techniques (Quantization, Distillation, Pruning) using **GPT-2** as the base model. \n",
                "\n",
                "### Why use this framework?\n",
                "1. **Modular**: Easily add new algorithms by wrapping them in a standard class.\n",
                "2. **Controlled**: Benchmarks all models on the same hardware and the same prompts.\n",
                "3. **Comprehensive**: Measures size, latency, and generation quality simultaneously."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers datasets torch bitsandbytes accelerate -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import time\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BitsAndBytesConfig\n",
                "import torch.nn.utils.prune as prune\n",
                "\n",
                "# Set device\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ†Ô∏è Utility Functions\n",
                "Standardized ways to measure performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_model_size(model):\n",
                "    param_size = 0\n",
                "    for param in model.parameters():\n",
                "        param_size += param.nelement() * param.element_size()\n",
                "    buffer_size = 0\n",
                "    for buffer in model.buffers():\n",
                "        buffer_size += buffer.nelement() * buffer.element_size()\n",
                "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
                "    return size_all_mb\n",
                "\n",
                "def benchmark_model(model, tokenizer, prompt=\"The future of artificial intelligence is\", max_new_tokens=30):\n",
                "    model.to(device)\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    # Warm-up\n",
                "    _ = model.generate(**inputs, max_new_tokens=5)\n",
                "    \n",
                "    start_time = time.time()\n",
                "    with torch.no_grad():\n",
                "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, min_new_tokens=max_new_tokens)\n",
                "    end_time = time.time()\n",
                "    \n",
                "    latency = end_time - start_time\n",
                "    tokens_generated = max_new_tokens\n",
                "    throughput = tokens_generated / latency\n",
                "    \n",
                "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
                "    return {\n",
                "        \"latency\": latency,\n",
                "        \"throughput\": throughput,\n",
                "        \"size_mb\": get_model_size(model),\n",
                "        \"sample_output\": decoded\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèóÔ∏è The Framework\n",
                "We loop through various models and collect data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
                "\n",
                "# 1. Baseline (FP32)\n",
                "print(\"Benchmarking Base Model...\")\n",
                "model_base = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
                "res_base = benchmark_model(model_base, tokenizer)\n",
                "res_base[\"method\"] = \"Baseline (FP32)\"\n",
                "results.append(res_base)\n",
                "del model_base\n",
                "\n",
                "# 2. Dynamic Quantization (INT8)\n",
                "print(\"Benchmarking Dynamic Quantization...\")\n",
                "model_quant = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cpu\") # Dynamic quant usually on CPU\n",
                "model_quant = torch.quantization.quantize_dynamic(model_quant, {torch.nn.Linear}, dtype=torch.qint8)\n",
                "# Benchmarking on CPU specifically for comparison\n",
                "orig_device = device\n",
                "device = \"cpu\"\n",
                "res_quant = benchmark_model(model_quant, tokenizer)\n",
                "res_quant[\"method\"] = \"Quantization (INT8-CPU)\"\n",
                "results.append(res_quant)\n",
                "device = orig_device\n",
                "del model_quant\n",
                "\n",
                "# 3. Pruning (30% Unstructured)\n",
                "print(\"Benchmarking Pruning...\")\n",
                "model_prune = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
                "for name, module in model_prune.named_modules():\n",
                "    if isinstance(module, torch.nn.Linear):\n",
                "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
                "res_prune = benchmark_model(model_prune, tokenizer)\n",
                "res_prune[\"method\"] = \"Pruning (30% L1)\"\n",
                "results.append(res_prune)\n",
                "del model_prune"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Comparison Dashboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.DataFrame(results)\n",
                "display(df[[\"method\", \"size_mb\", \"latency\", \"throughput\"]])\n",
                "\n",
                "# Visualizations\n",
                "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "df.plot(x=\"method\", y=\"size_mb\", kind=\"bar\", ax=ax[0], title=\"Model Size (MB) - Lower is Better\", color=\"skyblue\")\n",
                "df.plot(x=\"method\", y=\"throughput\", kind=\"bar\", ax=ax[1], title=\"Throughput (Tokens/sec) - Higher is Better\", color=\"salmon\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Add Your Own Algorithm\n",
                "To test a new method, simply load your processed model and call `benchmark_model(my_model, tokenizer)`."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}