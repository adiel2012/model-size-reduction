{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "setup-cell"
            },
            "outputs": [],
            "source": [
                "# @title 1. Universal Setup (Run All Compatible)\n",
                "# @markdown This cell fixes Google Colab environment issues. Run this first.\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "def is_colab():\n",
                "    return 'google.colab' in sys.modules or 'google.colab' in sys.builtin_module_names or os.path.exists('/content')\n",
                "\n",
                "if is_colab():\n",
                "    print(\"üåê Running in Google Colab. Validating environment...\")\n",
                "\n",
                "    # Check NumPy version without importing it into the main process to avoid crashing if it's broken\n",
                "    try:\n",
                "        np_version = subprocess.check_output([sys.executable, \"-c\", \"import numpy; print(numpy.__version__)\"], stderr=subprocess.STDOUT).decode().strip()\n",
                "        major_v = int(np_version.split('.')[0])\n",
                "    except Exception:\n",
                "        major_v = 0 # Assume broken/old\n",
                "\n",
                "    if major_v < 2:\n",
                "        print(f\"‚è´ Upgrading environment (Found NumPy {np_version if 'np_version' in locals() else 'broken'})...\")\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--upgrade\", \"numpy>=2.0\", \"tensorflow-model-optimization\", \"pandas\", \"matplotlib\", \"tabulate\"])\n",
                "        print(\"\\n‚ö†Ô∏è RESTARTING SESSION: A runtime reset is required to apply the NumPy upgrade.\")\n",
                "        print(\"Click 'Run All' again after the restart is complete (usually takes 5 seconds).\")\n",
                "        # The most reliable way to restart a Colab session programmatically for 'Run All' flow\n",
                "        os.kill(os.getpid(), 9)\n",
                "\n",
                "    # If we are here, NumPy is correct. Ensure TF MOT exists.\n",
                "    try:\n",
                "        import tensorflow_model_optimization\n",
                "    except ImportError:\n",
                "        print(\"üì¶ Installing experiment libraries...\")\n",
                "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"tensorflow-model-optimization\", \"pandas\", \"matplotlib\", \"tabulate\"])\n",
                "\n",
                "print(\"‚úÖ Environment Ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ The Ultimate Quantization Benchmark: Research to Production\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/experiment_framework.ipynb)\n",
                "\n",
                "## üìñ Overview\n",
                "This notebook provides a unified experimentation framework to compare the major quantization milestones from 2022 to 2026. While the chronology folders contain \"from scratch\" implementations for learning, this framework uses **TensorFlow (TFLite & TFMOT)** built-in functions to simulate these algorithms in a production-ready environment.\n",
                "\n",
                "### Algorithms Compared\n",
                "1.  **Baseline (FP32)**: The uncompressed reference model.\n",
                "2.  **LLM.int8() style**: Dynamic Range Quantization (Weight INT8).\n",
                "3.  **GPTQ / AWQ style**: Full Integer Quantization (Calibrated INT8).\n",
                "4.  **NF4 / HQQ style**: 4-bit Weight-only Quantization.\n",
                "5.  **BitNet / T-Poti style**: Simulated ultra-low precision (Sparsity + Quantization).\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Initialize TensorFlow and Checks\n",
                "import warnings\n",
                "import os\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import time\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow_model_optimization as tfmot\n",
                "\n",
                "# Silence warnings\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
                "warnings.filterwarnings('ignore', category=UserWarning)\n",
                "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
                "\n",
                "print(\"üöÄ TensorFlow version:\", tf.__version__)\n",
                "device_name = tf.test.gpu_device_name()\n",
                "if device_name != '/device:GPU:0':\n",
                "  print('‚ö†Ô∏è GPU not found! Benchmarking on CPU will be slower.')\n",
                "else:\n",
                "  print('‚úÖ Found GPU at: {}'.format(device_name))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Running the Experiment\n",
                "We will now programmatically convert the model using different strategies and measure the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Benchmark Logic\n",
                "results = []\n",
                "\n",
                "def create_benchmark_model():\n",
                "    model = tf.keras.Sequential([\n",
                "        tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
                "        tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
                "        tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
                "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
                "        tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
                "        tf.keras.layers.Flatten(),\n",
                "        tf.keras.layers.Dense(10)\n",
                "    ])\n",
                "    return model\n",
                "\n",
                "base_model = create_benchmark_model()\n",
                "base_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
                "\n",
                "# Load data\n",
                "mnist = tf.keras.datasets.mnist\n",
                "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
                "train_images = train_images.astype(np.float32) / 255.0\n",
                "test_images = test_images.astype(np.float32) / 255.0\n",
                "\n",
                "def representative_data_gen():\n",
                "    # Yield small batches of float32 images with batch dimension for TFLite representative data\n",
                "    max_samples = min(100, train_images.shape[0])\n",
                "    for i in range(max_samples):\n",
                "        img = train_images[i:i+1].astype(np.float32)\n",
                "        yield [img]\n",
                "\n",
                "\n",
                "def run_benchmark(model_content, name):\n",
                "    file_name = f\"{name}.tflite\"\n",
                "    with open(file_name, \"wb\") as f: f.write(model_content)\n",
                "    size_kb = os.path.getsize(file_name) / 1024\n",
                "    \n",
                "    interpreter = tf.lite.Interpreter(model_content=model_content)\n",
                "    interpreter.allocate_tensors()\n",
                "    input_details = interpreter.get_input_details()\n",
                "    output_details = interpreter.get_output_details()\n",
                "    input_idx = input_details[0]['index']\n",
                "    output_idx = output_details[0]['index']\n",
                "\n",
                "    # Prepare a single input matching expected shape/dtype\n",
                "    sample_input = test_images[0:1]\n",
                "    expected_shape = input_details[0].get('shape', None)\n",
                "    if expected_shape is not None and len(expected_shape) == 4 and sample_input.ndim == 3:\n",
                "        sample_input = np.expand_dims(sample_input, -1)\n",
                "\n",
                "    # Warmup\n",
                "    try:\n",
                "        interpreter.set_tensor(input_idx, sample_input.astype(input_details[0]['dtype']))\n",
                "        interpreter.invoke()\n",
                "    except Exception:\n",
                "        # Fallback: cast to float32\n",
                "        interpreter.set_tensor(input_idx, sample_input.astype(np.float32))\n",
                "        interpreter.invoke()\n",
                "\n",
                "    # Latency\n",
                "    start = time.time()\n",
                "    runs = 200\n",
                "    for _ in range(runs):\n",
                "        interpreter.set_tensor(input_idx, sample_input.astype(input_details[0]['dtype']))\n",
                "        interpreter.invoke()\n",
                "    latency_ms = (time.time() - start) / runs * 1000.0\n",
                "\n",
                "    # Accuracy\n",
                "    correct = 0\n",
                "    total = min(500, test_images.shape[0])\n",
                "\n",
                "    # Prepare evaluation inputs according to input dtype and quantization\n",
                "    dtype = input_details[0]['dtype']\n",
                "    quant = input_details[0].get('quantization', ())\n",
                "\n",
                "    if dtype == np.int8 and len(quant) >= 2:\n",
                "        scale, zero_point = quant\n",
                "        imgs = test_images[:total]\n",
                "        if len(imgs.shape) == 3 and expected_shape is not None and len(expected_shape) == 4:\n",
                "            imgs = np.expand_dims(imgs, -1)\n",
                "        if scale == 0:\n",
                "            imgs_q = imgs.astype(np.int8)\n",
                "        else:\n",
                "            imgs_q = np.round(imgs / scale + zero_point).astype(np.int8)\n",
                "    else:\n",
                "        imgs_q = test_images[:total]\n",
                "        if len(imgs_q.shape) == 3 and expected_shape is not None and len(expected_shape) == 4:\n",
                "            imgs_q = np.expand_dims(imgs_q, -1)\n",
                "\n",
                "    for i in range(total):\n",
                "        inp = imgs_q[i:i+1]\n",
                "        try:\n",
                "            interpreter.set_tensor(input_idx, inp.astype(dtype))\n",
                "        except Exception:\n",
                "            interpreter.set_tensor(input_idx, inp.astype(np.float32))\n",
                "        interpreter.invoke()\n",
                "        output = interpreter.get_tensor(output_idx)\n",
                "        prediction = np.argmax(output)\n",
                "        if prediction == int(test_labels[i]):\n",
                "            correct += 1\n",
                "\n",
                "    accuracy = (correct / total) * 100\n",
                "    return {\"Algorithm\": name, \"Size (KB)\": size_kb, \"Latency (ms)\": latency_ms, \"Accuracy (%)\": accuracy}\n",
                "\n",
                "print(\"üöÄ Starting experiments...\")\n",
                "\n",
                "# Execution\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "results.append(run_benchmark(conv.convert(), \"Baseline_FP32\"))\n",
                "\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "results.append(run_benchmark(conv.convert(), \"LLM_int8_Dynamic\"))\n",
                "\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "conv.representative_dataset = representative_data_gen\n",
                "conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
                "conv.inference_input_type = tf.int8\n",
                "conv.inference_output_type = tf.int8\n",
                "results.append(run_benchmark(conv.convert(), \"GPTQ_AWQ_FullInt\"))\n",
                "\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "try:\n",
                "    conv._experimental_new_quantizer = True\n",
                "except Exception:\n",
                "    pass\n",
                "results.append(run_benchmark(conv.convert(), \"NF4_HQQ_4bit\"))\n",
                "\n",
                "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
                "pruned_model = prune_low_magnitude(base_model, tfmot.sparsity.keras.ConstantSparsity(0.5, 0))\n",
                "pruned_model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
                "# Strip pruning wrappers before TFLite conversion\n",
                "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
                "conv = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
                "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
                "results.append(run_benchmark(conv.convert(), \"BitNet_TPoti_Extreme\"))\n",
                "\n",
                "print(\"‚úÖ All experiments complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Results & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.DataFrame(results)\n",
                "print(\"\\n--- Final Comparison Table ---\")\n",
                "print(df.to_markdown(index=False))\n",
                "\n",
                "fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(12, 10))\n",
                "ax1.set_ylabel('Size (KB)', color='tab:red')\n",
                "ax1.bar(df['Algorithm'], df['Size (KB)'], color='tab:red', alpha=0.3)\n",
                "ax1.set_xticklabels(df['Algorithm'], rotation=30)\n",
                "ax2 = ax1.twinx()\n",
                "ax2.set_ylabel('Latency (ms)', color='tab:blue')\n",
                "ax2.plot(df['Algorithm'], df['Latency (ms)'], color='tab:blue', marker='o')\n",
                "ax1.set_title('Size vs Latency')\n",
                "\n",
                "ax3.set_ylabel('Accuracy (%)', color='tab:green')\n",
                "ax3.bar(df['Algorithm'], df['Accuracy (%)'], color='tab:green', alpha=0.5)\n",
                "ax3.set_ylim(min(df['Accuracy (%)']) - 2, 100)\n",
                "ax3.set_xticklabels(df['Algorithm'], rotation=30)\n",
                "ax3.set_title('Accuracy')\n",
                "\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
