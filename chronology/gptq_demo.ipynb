{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª GPTQ: Accurate Post-Training Quantization (2022/2023)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-quantization/blob/main/chronology/gptq_demo.ipynb)\n",
                "\n",
                "GPTQ is a one-shot weight quantization method based on approximate second-order information. It is highly efficient and can quantize massive models in just a few GPU hours while maintaining high accuracy.\n",
                "\n",
                "In this notebook, we use `AutoGPTQ` to quantize an `OPT-125M` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install auto-gptq transformers optimum -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
                "import torch\n",
                "import time\n",
                "import os\n",
                "\n",
                "model_id = \"facebook/opt-125m\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "# 1. Load Model with GPTQ Configuration\n",
                "# We'll use 4-bit quantization with a group size of 128\n",
                "quantization_config = GPTQConfig(\n",
                "    bits=4, \n",
                "    group_size=128, \n",
                "    dataset=\"wikitext2\", \n",
                "    desc_act=False\n",
                ")\n",
                "\n",
                "print(\"--- Loading and Quantizing Model ---\")\n",
                "model_gptq = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id, \n",
                "    quantization_config=quantization_config, \n",
                "    torch_dtype=torch.float16, \n",
                "    device_map=\"auto\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_inference(model, tokenizer, input_text=\"The future of AI is\"):\n",
                "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
                "    start_time = time.time()\n",
                "    with torch.no_grad():\n",
                "        output = model.generate(**inputs, max_new_tokens=30)\n",
                "    end_time = time.time()\n",
                "    \n",
                "    print(f\"Duration: {end_time - start_time:.4f}s\")\n",
                "    print(f\"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}\")\n",
                "\n",
                "print(\"--- GPTQ Inference ---\")\n",
                "benchmark_inference(model_gptq, tokenizer)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}