{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª GPTQ From Scratch: Optimal Brain Quantization (2023)\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/gptq_demo.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: OBQ & The Hessian Matrix\n",
                "\n",
                "GPTQ (Generalized Post-Training Quantization) is a high-performance 4-bit quantization method. It is based on **Optimal Brain Quantization (OBQ)**, which aims to minimize the error between the original weight and the quantized weight, weighted by the sensitivity of the layer.\n",
                "\n",
                "### The Objective Function\n",
                "We want to find $W_{quant}$ that minimizes squared error, but not all weights are equal. Some weights \"hurt\" more when rounded. This sensitivity is captured by the **Hessian matrix** $H$ ($H = 2XX^T$).\n",
                "\n",
                "$$\\min_{W_{quant}} ||W X - W_{quant} X||_2^2 \\approx \\min_{W_{quant}} \\sum_{i} (w_i - w_{quant,i})^2 H_{ii}$$\n",
                "\n",
                "### The Greedy Update\n",
                "GPTQ quantizes weights one by one. After quantizing weight $w_i$, it adjusts the *remaining* weights to compensate for the introduced error:\n",
                "\n",
                "$$\\delta w = -(w_i - round(w_i)) \\cdot \\frac{1}{[H^{-1}]_{ii}} \\cdot [H^{-1}]_{:,i}$$\n",
                "\n",
                "Where $H^{-1}$ is the inverse Hessian. This \"compensatory\" step is what makes GPTQ so accurate at 4-bits.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class GPTQManual:\n",
                "    def __init__(self, layer):\n",
                "        self.layer = layer\n",
                "        self.dev = layer.weight.device\n",
                "        W = layer.weight.data.clone()\n",
                "        self.rows = W.shape[0]\n",
                "        self.columns = W.shape[1]\n",
                "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
                "        self.nsamples = 0\n",
                "\n",
                "    def add_batch(self, inp):\n",
                "        # Accumulate Hessian information from input activations\n",
                "        if len(inp.shape) == 2:\n",
                "            inp = inp.unsqueeze(0)\n",
                "        tmp = inp.shape[0]\n",
                "        if isinstance(self.layer, nn.Linear):\n",
                "            if len(inp.shape) == 3:\n",
                "                inp = inp.reshape((-1, inp.shape[-1]))\n",
                "            inp = inp.t()\n",
                "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
                "        self.nsamples += tmp\n",
                "        inp = inp.float()\n",
                "        self.H += 2 / self.nsamples * torch.matmul(inp, inp.t())\n",
                "\n",
                "    def quantize(self, bits=4):\n",
                "        W = self.layer.weight.data.clone().float()\n",
                "        H = self.H\n",
                "        \n",
                "        # Cholesky decomposition to stably invert the Hessian\n",
                "        dead = torch.diag(H) == 0\n",
                "        H[dead, dead] = 1\n",
                "        W[:, dead] = 0\n",
                "        \n",
                "        # Regularization to make it invertible\n",
                "        reg = 0.01 * torch.mean(torch.diag(H))\n",
                "        H += reg * torch.eye(self.columns, device=self.dev)\n",
                "        H_inv = torch.linalg.inv(H)\n",
                "        \n",
                "        # Manual Quantization Loop\n",
                "        Q = torch.zeros_like(W)\n",
                "        \n",
                "        # For demonstration: Symmetric Min-Max Quantization\n",
                "        scale = (torch.max(torch.abs(W)) / (2**(bits-1) - 1))\n",
                "        \n",
                "        print(f\"Starting GPTQ loop for {self.columns} columns...\")\n",
                "        for i in range(self.columns):\n",
                "            w = W[:, i]\n",
                "            # 1. Quantize weight\n",
                "            q = torch.round(w / scale).clamp(-(2**(bits-1)), 2**(bits-1)-1)\n",
                "            q_val = q * scale\n",
                "            Q[:, i] = q_val\n",
                "            \n",
                "            # 2. Error\n",
                "            err = (w - q_val) / H_inv[i, i]\n",
                "            \n",
                "            # 3. Compensate remaining weights (the OBQ update)\n",
                "            W[:, i:] -= err.unsqueeze(1) * H_inv[i, i:].unsqueeze(0)\n",
                "            \n",
                "        self.layer.weight.data = Q.to(self.layer.weight.dtype)\n",
                "        print(\"Quantization Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Test implementation\n",
                "layer = nn.Linear(128, 256)\n",
                "original_weight = layer.weight.data.clone()\n",
                "\n",
                "gptq = GPTQManual(layer)\n",
                "# Simulate 10 batches of calibration data\n",
                "for _ in range(10):\n",
                "    calibration_data = torch.randn(1, 128)\n",
                "    gptq.add_batch(calibration_data)\n",
                "\n",
                "gptq.quantize(bits=4)\n",
                "\n",
                "unique_vals = torch.unique(layer.weight.data).shape[0]\n",
                "print(f\"Unique values in weight matrix: {unique_vals} (Expected ~16 for 4-bit)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}