{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª GPTQ From Scratch: Optimal Brain Quantization (2023)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/gptq_demo.ipynb)\n",
    "\n",
    "## ðŸ“– The Theory: OBQ & The Hessian Matrix\n",
    "\n",
    "GPTQ (Generalized Post-Training Quantization) is a high-performance 4-bit quantization method. It is based on **Optimal Brain Quantization (OBQ)**, which aims to minimize the error between the original weight and the quantized weight, weighted by the sensitivity of the layer.\n",
    "\n",
    "### The Objective Function\n",
    "We want to find $W_{quant}$ that minimizes squared error, but not all weights are equal. Some weights \"hurt\" more when rounded. This sensitivity is captured by the **Hessian matrix** $H$ ($H = 2XX^T$).\n",
    "\n",
    "$$\\min_{W_{quant}} ||W X - W_{quant} X||_2^2 \\approx \\min_{W_{quant}} \\sum_{i} (w_i - w_{quant,i})^2 H_{ii}$$\n",
    "\n",
    "### The Greedy Update\n",
    "GPTQ quantizes weights one by one. After quantizing weight $w_i$, it adjusts the *remaining* weights to compensate for the introduced error:\n",
    "\n",
    "$$\\delta w = -(w_i - round(w_i)) \\cdot \\frac{1}{[H^{-1}]_{ii}} \\cdot [H^{-1}]_{:,i}$$\n",
    "\n",
    "Where $H^{-1}$ is the inverse Hessian. This \"compensatory\" step is what makes GPTQ so accurate at 4-bits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTQManual:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "        self.dev = layer.weight.device\n",
    "        W = layer.weight.data.clone()\n",
    "        self.rows = W.shape[0]\n",
    "        self.columns = W.shape[1]\n",
    "        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n",
    "        self.nsamples = 0\n",
    "\n",
    "    def add_batch(self, inp):\n",
    "        # Accumulate Hessian information from input activations\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        tmp = inp.shape[0]\n",
    "        if isinstance(self.layer, nn.Linear):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "            inp = inp.t()\n",
    "        self.H *= self.nsamples / (self.nsamples + tmp)\n",
    "        self.nsamples += tmp\n",
    "        inp = inp.float()\n",
    "        self.H += 2 / self.nsamples * torch.matmul(inp, inp.t())\n",
    "\n",
    "    def quantize(self, bits=4):\n",
    "        W = self.layer.weight.data.clone().float()\n",
    "        H = self.H\n",
    "        \n",
    "        # Cholesky decomposition to stably invert the Hessian\n",
    "        dead = torch.diag(H) == 0\n",
    "        H[dead, dead] = 1\n",
    "        W[:, dead] = 0\n",
    "        \n",
    "        # Regularization to make it invertible\n",
    "        reg = 0.01 * torch.mean(torch.diag(H))\n",
    "        H += reg * torch.eye(self.columns, device=self.dev)\n",
    "        H_inv = torch.linalg.inv(H)\n",
    "        \n",
    "        # Manual Quantization Loop\n",
    "        Q = torch.zeros_like(W)\n",
    "        \n",
    "        # For demonstration: Symmetric Min-Max Quantization\n",
    "        scale = (torch.max(torch.abs(W)) / (2**(bits-1) - 1))\n",
    "        \n",
    "        print(f\"Starting GPTQ loop for {self.columns} columns...\")\n",
    "        for i in range(self.columns):\n",
    "            w = W[:, i]\n",
    "            # 1. Quantize weight\n",
    "            q = torch.round(w / scale).clamp(-(2**(bits-1)), 2**(bits-1)-1)\n",
    "            q_val = q * scale\n",
    "            Q[:, i] = q_val\n",
    "            \n",
    "            # 2. Error\n",
    "            err = (w - q_val) / H_inv[i, i]\n",
    "            \n",
    "            # 3. Compensate remaining weights (the OBQ update)\n",
    "            W[:, i:] -= err.unsqueeze(1) * H_inv[i, i:].unsqueeze(0)\n",
    "            \n",
    "        self.layer.weight.data = Q.to(self.layer.weight.dtype)\n",
    "        print(\"Quantization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test implementation\n",
    "layer = nn.Linear(128, 256)\n",
    "original_weight = layer.weight.data.clone()\n",
    "\n",
    "gptq = GPTQManual(layer)\n",
    "# Simulate 10 batches of calibration data\n",
    "for _ in range(10):\n",
    "    calibration_data = torch.randn(1, 128)\n",
    "    gptq.add_batch(calibration_data)\n",
    "\n",
    "gptq.quantize(bits=4)\n",
    "\n",
    "unique_vals = torch.unique(layer.weight.data).shape[0]\n",
    "print(f\"Unique values in weight matrix: {unique_vals} (Expected ~16 for 4-bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: GPTQ OBQ update on a single weight row [w0, w1, w2]\nimport torch\n\nprint(\"=== GPTQ: column-by-column compensation on 3 weights ===\\n\")\n\nW = torch.tensor([[0.6, -0.4, 0.8]])          # 1 output Ã— 3 inputs\nprint(f\"Original weights W = {W.tolist()[0]}\")\n\n# 5 calibration input samples (3 features)\nX = torch.tensor([\n    [1.2, 0.3, 0.8],\n    [0.5, 1.8, 0.2],\n    [0.9, 0.4, 1.5],\n    [0.3, 1.1, 0.7],\n    [1.0, 0.6, 0.9],\n])\n# Hessian H = 2 * X^T @ X\nH = 2 * X.t() @ X\nprint(f\"\\nHessian H = 2Â·X^TÂ·X\")\nprint(H.numpy().round(3))\nprint(f\"H diagonal (sensitivity): {H.diag().tolist()}\")\nprint(f\"  â†’ w[2] is most sensitive (H[2,2]={H[2,2]:.2f}) â€” quantizing it badly hurts most\")\n\nH_inv = torch.linalg.inv(H + 0.01 * H.diag().mean() * torch.eye(3))\n\nbits  = 4\nscale = W.abs().max() / (2**(bits-1) - 1)\nW_q   = W.clone().float()\nQ     = torch.zeros_like(W_q)\n\nprint(f\"\\n4-bit scale = {scale:.4f}\")\nprint(f\"\\n{'Col':>4}  {'w_orig':>8}  {'q':>8}  {'round_err':>10}  {'compensation on remaining cols'}\")\nfor i in range(3):\n    w_i = W_q[0, i]\n    q_i = (w_i / scale).round().clamp(-(2**(bits-1)), 2**(bits-1)-1) * scale\n    Q[0, i] = q_i\n    err = (w_i - q_i) / H_inv[i, i]\n    comp = err * H_inv[i, i:]\n    W_q[0, i:] -= comp\n    print(f\"{i:>4}  {w_i.item():+8.4f}  {q_i.item():+8.4f}  {(w_i-q_i).item():+10.4f}  {comp.tolist()}\")\n\nprint(f\"\\nOriginal : {W.tolist()[0]}\")\nprint(f\"Quantized: {Q.tolist()[0]}\")\nprint(f\"Per-elem error: {(W - Q).abs().tolist()[0]}\")\nprint(f\"\\nWithout OBQ compensation a naive quantizer would give error â‰ˆ {(W.abs()%scale).mean().item():.4f}/weight;\")\nprint(f\"with compensation, later weights absorb the rounding error of earlier ones.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\n# GPTQ-style 4-bit quantization (min-max rounding, the inner quantization step of GPTQ)\nbits = 4\nmodel_q = copy.deepcopy(model)\nfor name, param in model_q.named_parameters():\n    if param.dim() == 2:\n        W = param.data.float()\n        scale = torch.max(torch.abs(W)) / (2**(bits - 1) - 1)\n        W_q = torch.round(W / scale).clamp(-(2**(bits - 1)), 2**(bits - 1) - 1) * scale\n        param.data = W_q.to(param.dtype)\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"GPTQ GPT-2 Perplexity:     {quant_ppl:.2f}\")\nprint(f\"Delta:                     {quant_ppl - baseline_ppl:+.2f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}