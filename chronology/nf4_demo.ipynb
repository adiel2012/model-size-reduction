{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß™ NF4 From Scratch: NormalFloat 4-bit Quantization (2023)\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/nf4_demo.ipynb)\n",
                "\n",
                "## üìñ The Theory: Information-Theoretic Optimality\n",
                "\n",
                "NF4 (NormalFloat 4) is a core component of QLoRA. Most weights in neural networks follow a **normal distribution** $\\mathcal{N}(0, \\sigma^2)$. Fixed-point quantization (like Regular INT4) is suboptimal for these distributions because it assigns equal space to values that are rare (near the tails) and values that are common (near zero).\n",
                "\n",
                "### Quantile Quantization\n",
                "NF4 defines 16 levels based on the **quantiles of the standard normal distribution**. Each level represents an equal probability mass under the Gaussian curve. This ensures that the 16 available \"buckets\" are used as efficiently as possible for normally distributed data.\n",
                "\n",
                "### Double Quantization\n",
                "In addition to NF4 weights, QLoRA quantizes the **quantization constants** (scales) themselves from 32-bit floats to 8-bit floats, saving an additional 0.37 bits per parameter on average.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "from scipy.stats import norm\n",
                "\n",
                "def create_nf4_map():\n",
                "    \"\"\"Manual creation of the NF4 16-level lookup table\"\"\"\n",
                "    # Standard normal distribution quantiles\n",
                "    # We need 16 values. QLoRA specifically uses a zero-centered asymmetric map.\n",
                "    offset = 1.0 / (2 * 16)\n",
                "    p_values = torch.linspace(offset, 1 - offset, 16)\n",
                "    \n",
                "    # Correct for NF4 specifics: it uses zero as one level and is symmetric at certain points\n",
                "    # This is a simplified version of the official NF4 constant list\n",
                "    nf4_values = norm.ppf(p_values)\n",
                "    nf4_values = torch.from_numpy(nf4_values).float()\n",
                "    \n",
                "    # Normalize to [-1, 1]\n",
                "    nf4_values = nf4_values / nf4_values.max()\n",
                "    return nf4_values.sort()[0]\n",
                "\n",
                "nf4_map = create_nf4_map()\n",
                "print(f\"NF4 Lookup Table (16 levels):\\n{nf4_map}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ†Ô∏è Implementation: Manual NF4 Mapping\n",
                "\n",
                "Let's implement the mapping from FP32 to the closest NF4 level."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def quantize_nf4(w, nf4_map):\n",
                "    \"\"\"\n",
                "    Quantize a weight matrix to the closest NF4 value.\n",
                "    w: Tensor in the range [-1, 1]\n",
                "    \"\"\"\n",
                "    # 1. Normalize weight to unit range if it isn't already\n",
                "    abs_max = torch.max(torch.abs(w))\n",
                "    w_norm = w / abs_max\n",
                "    \n",
                "    # 2. Find closest values in map\n",
                "    # This can be done efficiently with searchsorted or absolute difference\n",
                "    # For clarity, we use the difference method here\n",
                "    w_flat = w_norm.view(-1, 1)\n",
                "    diff = torch.abs(w_flat - nf4_map.view(1, -1))\n",
                "    indices = torch.argmin(diff, dim=1)\n",
                "    \n",
                "    # 3. Simulate Dequantization\n",
                "    q_w = nf4_map[indices].view(w.shape)\n",
                "    return q_w * abs_max, indices\n",
                "\n",
                "# Test with Normal data\n",
                "w_raw = torch.randn(1024, 1024)\n",
                "w_nf4, w_indices = quantize_nf4(w_raw, nf4_map)\n",
                "\n",
                "error = (w_raw - w_nf4).pow(2).mean()\n",
                "print(f\"Mean Squared Error: {error:.6f}\")\n",
                "print(f\"Compression: 32-bit to 4-bit indices (8x smaller storage)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}