{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üß™ NF4 From Scratch: NormalFloat 4-bit Quantization (2023)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/nf4_demo.ipynb)\n\n## üìñ Theory: Information-Theoretic Optimality\n\nNF4 (NormalFloat 4) is the data type introduced in **QLoRA** (Dettmers et al., 2023).\nIt achieves **information-theoretically optimal** quantization for data drawn from a\nnormal distribution -- which is precisely what pre-trained neural-network weights follow.\n\n### Why Not INT4?\n\nStandard INT4 places its 16 levels **uniformly** across $[x_{\\min}, x_{\\max}]$.\nFor a Gaussian weight distribution, most probability mass sits near zero,\nso many uniformly-spaced levels fall in rarely-visited tails.\nNF4 fixes this by placing levels at the **quantiles** of $\\mathcal{N}(0,1)$:\n\n$$q_i = \\Phi^{-1}\\!\\left(\\frac{i + 0.5}{16}\\right), \\quad i = 0,\\dots,15$$\n\nwhere $\\Phi^{-1}$ is the inverse standard-normal CDF.\nEach bucket covers an equal $1/16$ probability slice -- no level is wasted on rare values.\n\n### Per-Block Absmax Normalization\n\nEach weight block is normalised by its absolute maximum before lookup:\n\n$$\\hat{w} = \\frac{w}{\\max|W|} \\in [-1,\\,1]$$\n\nthen mapped to the nearest NF4 level. The scale $\\max|W|$ is stored alongside the\n4-bit index and multiplied back at dequantization time.\n\n### Double Quantization\n\nQLoRA further compresses per-block scales from 32-bit floats to **8-bit floats**,\nsaving an additional $\\approx 0.37$ bits per parameter at negligible accuracy loss.\n\n### NF4 vs INT4 at a Glance\n\n| Property | INT4 | NF4 |\n|---|---|---|\n| Level placement | Uniform | Quantile-based |\n| Optimal for | Uniform distributions | Normal distributions |\n| Typical weight MSE | Higher | ~30% lower |\n| Used in | General PTQ | QLoRA fine-tuning |\n\n### Limitations\n* Assumes weights are **normally distributed** -- fails for bimodal or heavy-tailed layers.\n* Requires storing a per-block FP8 scale, adding a small memory overhead.\n* Dequantization introduces a lookup step that can be slower than pure INT arithmetic.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from scipy.stats import norm\n",
    "\n",
    "def create_nf4_map():\n",
    "    \"\"\"Manual creation of the NF4 16-level lookup table\"\"\"\n",
    "    # Standard normal distribution quantiles\n",
    "    # We need 16 values. QLoRA specifically uses a zero-centered asymmetric map.\n",
    "    offset = 1.0 / (2 * 16)\n",
    "    p_values = torch.linspace(offset, 1 - offset, 16)\n",
    "    \n",
    "    # Correct for NF4 specifics: it uses zero as one level and is symmetric at certain points\n",
    "    # This is a simplified version of the official NF4 constant list\n",
    "    nf4_values = norm.ppf(p_values)\n",
    "    nf4_values = torch.from_numpy(nf4_values).float()\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    nf4_values = nf4_values / nf4_values.max()\n",
    "    return nf4_values.sort()[0]\n",
    "\n",
    "nf4_map = create_nf4_map()\n",
    "print(f\"NF4 Lookup Table (16 levels):\\n{nf4_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Implementation: Manual NF4 Mapping\n",
    "\n",
    "Let's implement the mapping from FP32 to the closest NF4 level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def quantize_nf4(w, nf4_map):\n",
    "    \"\"\"\n",
    "    Quantize a weight matrix to the closest NF4 value.\n",
    "    w: Tensor in the range [-1, 1]\n",
    "    \"\"\"\n",
    "    # 1. Normalize weight to unit range if it isn't already\n",
    "    abs_max = torch.max(torch.abs(w))\n",
    "    w_norm = w / abs_max\n",
    "    \n",
    "    # 2. Find closest values in map\n",
    "    # This can be done efficiently with searchsorted or absolute difference\n",
    "    # For clarity, we use the difference method here\n",
    "    w_flat = w_norm.view(-1, 1)\n",
    "    diff = torch.abs(w_flat - nf4_map.view(1, -1))\n",
    "    indices = torch.argmin(diff, dim=1)\n",
    "    \n",
    "    # 3. Simulate Dequantization\n",
    "    q_w = nf4_map[indices].view(w.shape)\n",
    "    return q_w * abs_max, indices\n",
    "\n",
    "# Test with Normal data\n",
    "w_raw = torch.randn(1024, 1024)\n",
    "w_nf4, w_indices = quantize_nf4(w_raw, nf4_map)\n",
    "\n",
    "error = (w_raw - w_nf4).pow(2).mean()\n",
    "print(f\"Mean Squared Error: {error:.6f}\")\n",
    "print(f\"Compression: 32-bit to 4-bit indices (8x smaller storage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üî¢ Worked Example with Numbers\n\nBefore the full implementation, let‚Äôs trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: quantize 6 weights to NF4 step-by-step\n# (nf4_map and quantize_nf4 are defined in the cell above)\nimport torch\n\nw = torch.tensor([0.9, -0.3, 0.05, -0.85, 0.4, -0.1])\nprint(f\"Original weights: {[round(v,2) for v in w.tolist()]}\")\n\n# Step 1 ‚Äì Normalize to [-1, 1]\nabs_max = w.abs().max()\nw_norm  = w / abs_max\nprint(f\"\\nStep 1  Normalize (√∑{abs_max:.2f}):\")\nprint(f\"  {[round(v,4) for v in w_norm.tolist()]}\")\n\n# Step 2 ‚Äì Show the 16 NF4 levels\nprint(\"\\nStep 2  NF4 lookup table (16 levels):\")\nfor i, v in enumerate(nf4_map.tolist()):\n    print(f\"  [{i:2d}]  {v:+.4f}\")\n\n# Step 3 ‚Äì Map each normalized value to the nearest NF4 level\ndiff    = (w_norm.view(-1, 1) - nf4_map.view(1, -1)).abs()\nindices = diff.argmin(dim=1)\nw_q     = nf4_map[indices]\nprint(\"\\nStep 3  Nearest NF4 level + dequantize (√óabs_max):\")\nfor orig, nrm, idx, q in zip(w.tolist(), w_norm.tolist(), indices.tolist(), w_q.tolist()):\n    recon = q * abs_max.item()\n    err   = abs(orig - recon)\n    print(f\"  {orig:+.4f}  ‚Üí  norm {nrm:+.4f}  ‚Üí  NF4[{idx:2d}]={q:+.4f}  ‚Üí  recon {recon:+.4f}  (err {err:.4f})\")\n\nprint(f\"\\nMean absolute error : {(w - w_q*abs_max).abs().mean():.6f}\")\nprint(f\"Storage: 4-bit index (0-15) instead of 32-bit float  ‚Üí  8√ó smaller\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üß™ GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\nmodel_q = copy.deepcopy(model)\nfor name, param in model_q.named_parameters():\n    if param.dim() == 2:\n        q_w, _ = quantize_nf4(param.data, nf4_map)\n        param.data = q_w\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"NF4 GPT-2 Perplexity:      {quant_ppl:.2f}\")\nprint(f\"Delta:                     {quant_ppl - baseline_ppl:+.2f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö References\n\n1. **Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L.** (2023).  \n   *QLoRA: Efficient Finetuning of Quantized LLMs.* NeurIPS 2023.  \n   [arXiv:2305.14314](https://arxiv.org/abs/2305.14314)\n\n2. **Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L.** (2022).  \n   *The case for 4-bit precision: k-bit Inference Scaling Laws.* ICML 2023.  \n   [arXiv:2212.09720](https://arxiv.org/abs/2212.09720)\n\n3. **Hu, E., Shen, Y., Wallis, P., et al.** (2021).  \n   *LoRA: Low-Rank Adaptation of Large Language Models.* ICLR 2022.  \n   [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}