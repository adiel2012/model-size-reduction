{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª HQQ From Scratch: Half-Quadratic Quantization (2024)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/hqq_demo.ipynb)\n",
    "\n",
    "## ðŸ“– The Theory: Data-Free Optimization\n",
    "\n",
    "HQQ (Half-Quadratic Quantization) is a fast and robust quantization method that doesn't require **calibration data**. Unlike GPTQ or AWQ, which look at activations, HQQ treats quantization as a mathematical optimization problem based solely on the weights $W$.\n",
    "\n",
    "### The Objective\n",
    "We want to find a scale $S$ and quantized weights $Q$ such that:\n",
    "\n",
    "$$\\min_{S, Q} ||W - S \\cdot Q||_2^2$$\n",
    "\n",
    "### Iterative Solver\n",
    "HQQ uses an iterative approach. It alternates between:\n",
    "1.  **Estimating the Scale $S$**: Given $Q$, find the best $S$ that fits the weights (least squares).\n",
    "2.  **Quantizing $W$**: Given $S$, map $W$ to the closest integer values in $Q$.\n",
    "\n",
    "This converges extremely quickly (often in 2-3 iterations) to a much better solution than simple min-max rounding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def hqq_quantize_block(W, bits=4, n_iter=5):\n",
    "    \"\"\"\n",
    "    Simplified HQQ algorithm implementation from scratch.\n",
    "    W: Weight tensor (can be a layer or block)\n",
    "    \"\"\"\n",
    "    orig_shape = W.shape\n",
    "    W = W.view(-1).float()\n",
    "    \n",
    "    # Initialize Scale with simple min-max\n",
    "    q_max = 2**(bits-1) - 1\n",
    "    scale = (torch.max(torch.abs(W)) / q_max).item()\n",
    "    \n",
    "    q_w = torch.round(W / scale).clamp(-q_max, q_max)\n",
    "    \n",
    "    print(f\"Optimizing scale for {bits}-bit weight...\")\n",
    "    for i in range(n_iter):\n",
    "        # 1. Update Scale (Least Squares: S = (W Â· Q) / (Q Â· Q))\n",
    "        num = torch.dot(W, q_w)\n",
    "        den = torch.dot(q_w, q_w)\n",
    "        scale = (num / den).item()\n",
    "        \n",
    "        # 2. Update Quantized Weights\n",
    "        q_w = torch.round(W / scale).clamp(-q_max, q_max)\n",
    "        \n",
    "        # Calculate error\n",
    "        err = torch.norm(W - scale * q_w)\n",
    "        if i % 2 == 0:\n",
    "            print(f\"  Iteration {i}: MSE = {(err**2/len(W)):.8f}\")\n",
    "            \n",
    "    return q_w.view(orig_shape), scale\n",
    "\n",
    "# Test implementation\n",
    "W_raw = torch.randn(512, 512)\n",
    "W_q, s = hqq_quantize_block(W_raw, bits=4)\n",
    "\n",
    "final_w = W_q * s\n",
    "print(f\"\\nFinal Quantized Scale: {s:.6f}\")\n",
    "print(f\"Manual HQQ Reconstruction Mean Error: {torch.abs(W_raw - final_w).mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: HQQ iterative optimization on a 2Ã—2 matrix using 2-bit\nimport torch\n\nW_small = torch.tensor([[0.8, -0.3],\n                        [0.1, -0.9]])\nbits  = 2          # 4 levels: -1, 0, 1  (q_max = 1)\nq_max = 2**(bits-1) - 1\nW     = W_small.view(-1).float()\n\nprint(f\"Original weights (flattened): {W.tolist()}\")\nprint(f\"Bits={bits}  â†’  q_max={q_max}  â†’  levels {{{-q_max}â€¦{q_max}}}\")\n\n# Initial min-max scale\nscale = (W.abs().max() / q_max).item()\nq_w   = W.div(scale).round().clamp(-q_max, q_max)\nprint(f\"\\n{'Iter':>5}  {'scale':>8}  {'q_w':>20}  {'MSE':>12}\")\nprint(f\"{'init':>5}  {scale:8.4f}  {str(q_w.tolist()):>20}  {((W - q_w*scale)**2).mean().item():12.6f}\")\n\nfor i in range(1, 4):\n    # Least-squares scale update\n    scale = (W.dot(q_w) / q_w.dot(q_w)).item()\n    q_w   = W.div(scale).round().clamp(-q_max, q_max)\n    mse   = ((W - q_w * scale)**2).mean().item()\n    print(f\"{i:>5}  {scale:8.4f}  {str(q_w.tolist()):>20}  {mse:12.6f}\")\n\nrecon = (q_w * scale).view(2, 2)\nprint(f\"\\nOriginal   :\\n{W_small.numpy()}\")\nprint(f\"Reconstructed:\\n{recon.numpy().round(4)}\")\nprint(f\"Element-wise error:\\n{(W_small - recon).abs().numpy().round(4)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy, io, contextlib\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\nmodel_q = copy.deepcopy(model)\nfor name, param in model_q.named_parameters():\n    if param.dim() == 2:\n        with contextlib.redirect_stdout(io.StringIO()):\n            q_w, scale = hqq_quantize_block(param.data, bits=4, n_iter=3)\n        param.data = q_w * scale\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"HQQ GPT-2 Perplexity:      {quant_ppl:.2f}\")\nprint(f\"Delta:                     {quant_ppl - baseline_ppl:+.2f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}