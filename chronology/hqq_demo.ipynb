{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ§ª HQQ From Scratch: Half-Quadratic Quantization (2024)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/hqq_demo.ipynb)\n\n## ðŸ“– Theory: Data-Free Optimization\n\nHQQ quantizes without **calibration data**. Instead of inspecting activations\n(as GPTQ or AWQ do), it treats quantization as a pure weight-space optimization problem.\n\n### Objective Function\n\nFind a scale $S$ and quantized integer weights $Q$ such that:\n\n$$\\min_{S,\\,Q}\\; \\|W - S \\cdot Q\\|_2^2$$\n\nThis is a **bilinear** problem, so HQQ alternates between two sub-problems\n(coordinate descent / alternating least squares).\n\n### Iterative Solver\n\n**Step 1 -- Update Scale** (given $Q$, find best $S$ by least squares):\n\n$$S^* = \\frac{W \\cdot Q}{Q \\cdot Q} = \\frac{\\langle W, Q\\rangle}{\\|Q\\|^2}$$\n\n**Step 2 -- Update $Q$** (given $S$, round-and-clamp):\n\n$$Q = \\text{clamp}\\!\\left(\\text{round}\\!\\left(\\frac{W}{S}\\right),\\; -2^{b-1},\\; 2^{b-1}-1\\right)$$\n\nConvergence is **extremely fast** (2-5 iterations) because the optimal scale\nhas a closed form and rounding is idempotent once stable.\n\n### Half-Quadratic Origin\n\nThe name comes from *half-quadratic splitting* in image processing (Geman & Yang, 1995),\nwhich regularises non-smooth problems by introducing an auxiliary variable.\nHere $Q$ plays the role of that auxiliary variable, decoupling the integer\nconstraint from the least-squares optimisation.\n\n### HQQ vs GPTQ vs Naive INT4\n\n| Property | Naive INT4 | GPTQ | HQQ |\n|---|---|---|---|\n| Calibration data | No | Yes | **No** |\n| Weight MSE | Highest | Low | Low |\n| Speed | Instant | Slow | **Fast** |\n| Activation-aware | No | Yes | No |\n\n### Limitations\n* Ignores the downstream loss -- only minimises weight reconstruction MSE.\n* No activation-aware scaling; can miss outlier weight channels.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def hqq_quantize_block(W, bits=4, n_iter=5):\n",
    "    \"\"\"\n",
    "    Simplified HQQ algorithm implementation from scratch.\n",
    "    W: Weight tensor (can be a layer or block)\n",
    "    \"\"\"\n",
    "    orig_shape = W.shape\n",
    "    W = W.view(-1).float()\n",
    "    \n",
    "    # Initialize Scale with simple min-max\n",
    "    q_max = 2**(bits-1) - 1\n",
    "    scale = (torch.max(torch.abs(W)) / q_max).item()\n",
    "    \n",
    "    q_w = torch.round(W / scale).clamp(-q_max, q_max)\n",
    "    \n",
    "    print(f\"Optimizing scale for {bits}-bit weight...\")\n",
    "    for i in range(n_iter):\n",
    "        # 1. Update Scale (Least Squares: S = (W Â· Q) / (Q Â· Q))\n",
    "        num = torch.dot(W, q_w)\n",
    "        den = torch.dot(q_w, q_w)\n",
    "        scale = (num / den).item()\n",
    "        \n",
    "        # 2. Update Quantized Weights\n",
    "        q_w = torch.round(W / scale).clamp(-q_max, q_max)\n",
    "        \n",
    "        # Calculate error\n",
    "        err = torch.norm(W - scale * q_w)\n",
    "        if i % 2 == 0:\n",
    "            print(f\"  Iteration {i}: MSE = {(err**2/len(W)):.8f}\")\n",
    "            \n",
    "    return q_w.view(orig_shape), scale\n",
    "\n",
    "# Test implementation\n",
    "W_raw = torch.randn(512, 512)\n",
    "W_q, s = hqq_quantize_block(W_raw, bits=4)\n",
    "\n",
    "final_w = W_q * s\n",
    "print(f\"\\nFinal Quantized Scale: {s:.6f}\")\n",
    "print(f\"Manual HQQ Reconstruction Mean Error: {torch.abs(W_raw - final_w).mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: HQQ iterative optimization on a 2Ã—2 matrix using 2-bit\nimport torch\n\nW_small = torch.tensor([[0.8, -0.3],\n                        [0.1, -0.9]])\nbits  = 2          # 4 levels: -1, 0, 1  (q_max = 1)\nq_max = 2**(bits-1) - 1\nW     = W_small.view(-1).float()\n\nprint(f\"Original weights (flattened): {W.tolist()}\")\nprint(f\"Bits={bits}  â†’  q_max={q_max}  â†’  levels {{{-q_max}â€¦{q_max}}}\")\n\n# Initial min-max scale\nscale = (W.abs().max() / q_max).item()\nq_w   = W.div(scale).round().clamp(-q_max, q_max)\nprint(f\"\\n{'Iter':>5}  {'scale':>8}  {'q_w':>20}  {'MSE':>12}\")\nprint(f\"{'init':>5}  {scale:8.4f}  {str(q_w.tolist()):>20}  {((W - q_w*scale)**2).mean().item():12.6f}\")\n\nfor i in range(1, 4):\n    # Least-squares scale update\n    scale = (W.dot(q_w) / q_w.dot(q_w)).item()\n    q_w   = W.div(scale).round().clamp(-q_max, q_max)\n    mse   = ((W - q_w * scale)**2).mean().item()\n    print(f\"{i:>5}  {scale:8.4f}  {str(q_w.tolist()):>20}  {mse:12.6f}\")\n\nrecon = (q_w * scale).view(2, 2)\nprint(f\"\\nOriginal   :\\n{W_small.numpy()}\")\nprint(f\"Reconstructed:\\n{recon.numpy().round(4)}\")\nprint(f\"Element-wise error:\\n{(W_small - recon).abs().numpy().round(4)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy, io, contextlib\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\nmodel_q = copy.deepcopy(model)\nfor name, param in model_q.named_parameters():\n    if param.dim() == 2:\n        with contextlib.redirect_stdout(io.StringIO()):\n            q_w, scale = hqq_quantize_block(param.data, bits=4, n_iter=3)\n        param.data = q_w * scale\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"HQQ GPT-2 Perplexity:      {quant_ppl:.2f}\")\nprint(f\"Delta:                     {quant_ppl - baseline_ppl:+.2f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ“š References\n\n1. **Badri, H. & Shaji, A.** (2023).  \n   *Half-Quadratic Quantization of Large Machine Learning Models.*  \n   [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n\n2. **Geman, D. & Yang, C.** (1995).  \n   *Nonlinear Image Recovery with Half-Quadratic Regularization.*  \n   IEEE Transactions on Image Processing, 4(7), 932-946.\n\n3. **Frantar, E., Ashkboos, S., Hoefler, T., & Alistarh, D.** (2022).  \n   *GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.* ICLR 2023.  \n   [arXiv:2210.17323](https://arxiv.org/abs/2210.17323)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}