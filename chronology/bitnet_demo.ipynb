{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª BitNet 1.58b From Scratch: Ternary LLMs (2025)\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/bitnet_demo.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: The Era of 1-bit LLMs\n",
                "\n",
                "BitNet 1.58b is a landmark architecture where every parameter is limited to three possible values: `{-1, 0, 1}`. This is often called **1.58-bit** because $\\log_2(3) \\approx 1.58$.\n",
                "\n",
                "### Why Ternary?\n",
                "- **Addition-only Math**: Multiplying by `1` or `-1` is just adding or subtracting. Multiplying by `0` is doing nothing. This eliminates the \"Multiplication\" part of the Matrix-Multiply-Accumulate (MAC) operation, which is the most power-hungry part of modern chips.\n",
                "- **Hardware Efficiency**: DRAM access and computation units become significantly simpler and faster.\n",
                "\n",
                "### The Quantization Logic\n",
                "To convert a weight $W$ to ternary, we use $abs\\_max$ normalization:\n",
                "\n",
                "$$\\gamma = \\max(|W|)$$\n",
                "$$W_{quant} = \\text{round}(\\text{clip}(\\frac{W}{\\gamma + \\epsilon}, -1, 1))$$\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "def bitnet_quantize(W):\n",
                "    \"\"\"\n",
                "    Manual BitNet 1.58b quantization loop.\n",
                "    Scales and maps weights to {-1, 0, 1}.\n",
                "    \"\"\"\n",
                "    # 1. Calculate Gamma: mean absolute value is often used for better outlier resilience\n",
                "    # but the paper uses abs_max for strict ternary range.\n",
                "    gamma = torch.max(torch.abs(W))\n",
                "    \n",
                "    # 2. Scale and Round to ternary\n",
                "    # Note: we add a epsilon to avoid div by zero\n",
                "    W_scaled = W / (gamma + 1e-7)\n",
                "    W_quant = torch.round(torch.clamp(W_scaled, -1, 1)).to(torch.int8)\n",
                "    \n",
                "    return W_quant, gamma\n",
                "\n",
                "def bitnet_matmul(X, W_quant, gamma):\n",
                "    \"\"\"\n",
                "    Simulated BitNet inference.\n",
                "    X is assumed to be 8-bit quantized activations.\n",
                "    \"\"\"\n",
                "    # In hardware, this is an addition-only matrix multiplication\n",
                "    # Here, we represent it using integer matmul\n",
                "    result = torch.matmul(X.to(torch.float32), W_quant.to(torch.float32))\n",
                "    \n",
                "    # Rescale back to FP range\n",
                "    return result * gamma\n",
                "\n",
                "# Demonstration\n",
                "W = torch.randn(512, 1024)\n",
                "W_q, gamma = bitnet_quantize(W)\n",
                "\n",
                "print(f\"Original weight sample: {W[0, :3].tolist()}\")\n",
                "print(f\"Ternary weight sample:  {W_q[0, :3].tolist()}\")\n",
                "print(f\"Unique values in W_q: {torch.unique(W_q).tolist()}\")\n",
                "\n",
                "X = torch.randn(1, 512)\n",
                "out = bitnet_matmul(X, W_q, gamma)\n",
                "print(f\"\\nOutput Scale (Gamma): {gamma:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}