{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª BitNet 1.58b From Scratch: Ternary LLMs (2025)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/bitnet_demo.ipynb)\n",
    "\n",
    "## ðŸ“– The Theory: The Era of 1-bit LLMs\n",
    "\n",
    "BitNet 1.58b is a landmark architecture where every parameter is limited to three possible values: `{-1, 0, 1}`. This is often called **1.58-bit** because $\\log_2(3) \\approx 1.58$.\n",
    "\n",
    "### Why Ternary?\n",
    "- **Addition-only Math**: Multiplying by `1` or `-1` is just adding or subtracting. Multiplying by `0` is doing nothing. This eliminates the \"Multiplication\" part of the Matrix-Multiply-Accumulate (MAC) operation, which is the most power-hungry part of modern chips.\n",
    "- **Hardware Efficiency**: DRAM access and computation units become significantly simpler and faster.\n",
    "\n",
    "### The Quantization Logic\n",
    "To convert a weight $W$ to ternary, we use $abs\\_max$ normalization:\n",
    "\n",
    "$$\\gamma = \\max(|W|)$$\n",
    "$$W_{quant} = \\text{round}(\\text{clip}(\\frac{W}{\\gamma + \\epsilon}, -1, 1))$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def bitnet_quantize(W):\n",
    "    \"\"\"\n",
    "    Manual BitNet 1.58b quantization loop.\n",
    "    Scales and maps weights to {-1, 0, 1}.\n",
    "    \"\"\"\n",
    "    # 1. Calculate Gamma: mean absolute value is often used for better outlier resilience\n",
    "    # but the paper uses abs_max for strict ternary range.\n",
    "    gamma = torch.max(torch.abs(W))\n",
    "    \n",
    "    # 2. Scale and Round to ternary\n",
    "    # Note: we add a epsilon to avoid div by zero\n",
    "    W_scaled = W / (gamma + 1e-7)\n",
    "    W_quant = torch.round(torch.clamp(W_scaled, -1, 1)).to(torch.int8)\n",
    "    \n",
    "    return W_quant, gamma\n",
    "\n",
    "def bitnet_matmul(X, W_quant, gamma):\n",
    "    \"\"\"\n",
    "    Simulated BitNet inference.\n",
    "    X is assumed to be 8-bit quantized activations.\n",
    "    \"\"\"\n",
    "    # In hardware, this is an addition-only matrix multiplication\n",
    "    # Here, we represent it using integer matmul\n",
    "    result = torch.matmul(X.to(torch.float32), W_quant.to(torch.float32))\n",
    "    \n",
    "    # Rescale back to FP range\n",
    "    return result * gamma\n",
    "\n",
    "# Demonstration\n",
    "W = torch.randn(512, 1024)\n",
    "W_q, gamma = bitnet_quantize(W)\n",
    "\n",
    "print(f\"Original weight sample: {W[0, :3].tolist()}\")\n",
    "print(f\"Ternary weight sample:  {W_q[0, :3].tolist()}\")\n",
    "print(f\"Unique values in W_q: {torch.unique(W_q).tolist()}\")\n",
    "\n",
    "X = torch.randn(1, 512)\n",
    "out = bitnet_matmul(X, W_q, gamma)\n",
    "print(f\"\\nOutput Scale (Gamma): {gamma:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: BitNet 1.58b ternary quantization on a small vector\nimport torch\n\nw = torch.tensor([0.8, -0.3, 0.05, -0.7, 0.2, -0.95])\nprint(f\"Original weights: {[round(v,2) for v in w.tolist()]}\")\n\n# Step 1 â€“ Gamma = abs_max\ngamma = w.abs().max()\nprint(f\"\\nStep 1  Gamma (abs_max) = {gamma:.4f}\")\n\n# Step 2 â€“ Scale and clamp to [-1, 1], then round to {-1, 0, 1}\nw_scaled = w / (gamma + 1e-7)\nw_ternary = w_scaled.clamp(-1, 1).round().to(torch.int8)\nprint(\"\\nStep 2  Scale â†’ round â†’ ternary:\")\nprint(f\"  {'orig':>7}  {'Ã·gamma':>8}  {'round':>6}\")\nfor orig, sc, t in zip(w.tolist(), w_scaled.tolist(), w_ternary.tolist()):\n    print(f\"  {orig:+7.4f}  {sc:+8.4f}  {t:+6d}\")\n\n# Step 3 â€“ Dequantize (just multiply by gamma)\nw_recon = w_ternary.float() * gamma\nprint(f\"\\nStep 3  Reconstruct (ternary Ã— gamma={gamma:.4f}):\")\nprint(f\"  {'orig':>7}  {'ternary':>8}  {'recon':>8}  {'err':>8}\")\nfor orig, t, r in zip(w.tolist(), w_ternary.tolist(), w_recon.tolist()):\n    print(f\"  {orig:+7.4f}  {t:+8d}  {r:+8.4f}  {abs(orig-r):8.4f}\")\n\nprint(f\"\\nMSE: {((w - w_recon)**2).mean():.6f}\")\nprint(\"\\nHardware insight: multiply-by-ternary = add / subtract / skip\")\nprint(\"  +1 â†’ ADD     the weight row\")\nprint(\"   0 â†’ SKIP    (no operation)\")\nprint(\"  -1 â†’ SUBTRACT the weight row\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity:      {baseline_ppl:.2f}\")\n\nmodel_q = copy.deepcopy(model)\nfor name, param in model_q.named_parameters():\n    if param.dim() == 2:\n        w_q, gamma = bitnet_quantize(param.data)\n        param.data = w_q.float() * gamma\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"BitNet 1.58b GPT-2 Perplexity:  {quant_ppl:.2f}\")\nprint(f\"Delta:                          {quant_ppl - baseline_ppl:+.2f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}