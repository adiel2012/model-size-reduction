{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª LLM.int8() From Scratch: Outlier-Aware Quantization (2022)\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/llm_int8_demo.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: Vector-wise Quantization & Outliers\n",
                "\n",
                "Large Language Models (LLMs) exhibit a unique property: as they scale, certain feature dimensions in the activations develop **emergent outliers**. These outliers have magnitudes significantly larger than the rest of the values (often $>6.0$ while the mean is $<0.1$).\n",
                "\n",
                "### The Challenge\n",
                "Standard 8-bit quantization (INT8) maps a range $[min, max]$ to $[-128, 127]$. If a single outlier is $60.0$ and most values are $0.1$, the quantization scale becomes massive, crushing all the precision of the $0.1$ values into zero.\n",
                "\n",
                "### The LLM.int8() Solution\n",
                "LLM.int8() uses a two-part strategy:\n",
                "1.  **Vector-wise Quantization**: Each row/column of the weight matrix and activation matrix is quantized with its own scale factor.\n",
                "2.  **Outlier Handling**: Dimensions that contain outliers ($> \\text{threshold}$) are extracted and multiplied in high precision (**FP16/FP32**). The rest are multiplied in **INT8**.\n",
                "\n",
                "$$Y = X_{int8} \\cdot W_{int8} + X_{outlier} \\cdot W_{outlier}$$\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import time\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "\n",
                "# 1. Load Model & Tokenizer\n",
                "model_id = \"gpt2\"\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
                "model = GPT2LMHeadModel.from_pretrained(model_id).eval()\n",
                "\n",
                "def print_size_in_mb(tensor, name):\n",
                "    size = tensor.numel() * tensor.element_size() / (1024**2)\n",
                "    print(f\"{name} Size: {size:.2f} MB\")\n",
                "\n",
                "weight = model.transformer.h[0].attn.c_attn.weight.data\n",
                "print_size_in_mb(weight, \"Original FP32 Weight\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ› ï¸ Implementation: Manual Vector-wise Quantization\n",
                "Instead of using black-box libraries, we implement the core math manually to demonstrate the efficiency gains."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def llm_int8_matmul(X, W, threshold=6.0):\n",
                "    \"\"\"\n",
                "    A from-scratch simulation of outlier-aware matmul.\n",
                "    \"\"\"\n",
                "    # 1. Identify Outlier Dimensions\n",
                "    X = X.float()\n",
                "    W = W.float()\n",
                "    col_max = torch.max(torch.abs(X), dim=0)[0]\n",
                "    outlier_idx = torch.where(col_max > threshold)[0]\n",
                "    non_outlier_idx = torch.where(col_max <= threshold)[0]\n",
                "    \n",
                "    # 2. Extract and multiply Outliers in high precision\n",
                "    X_outlier = X[:, outlier_idx]\n",
                "    W_outlier = W[outlier_idx, :]\n",
                "    outlier_res = torch.matmul(X_outlier, W_outlier)\n",
                "    \n",
                "    # 3. Quantize and multiply others in INT8 simulation\n",
                "    X_non = X[:, non_outlier_idx]\n",
                "    W_non = W[non_outlier_idx, :]\n",
                "    \n",
                "    if X_non.shape[1] > 0:\n",
                "        # Vector-wise quant for X (row units)\n",
                "        x_scale = torch.max(torch.abs(X_non), dim=1, keepdim=True)[0] / 127\n",
                "        X_q = torch.round(X_non / (x_scale + 1e-8)).to(torch.int8)\n",
                "        \n",
                "        # Vector-wise quant for W (col units)\n",
                "        w_scale = torch.max(torch.abs(W_non), dim=0, keepdim=True)[0] / 127\n",
                "        W_q = torch.round(W_non / (w_scale + 1e-8)).to(torch.int8)\n",
                "        \n",
                "        # Matmul simulation (using Torch's matmul on float32 converted from int8)\n",
                "        int8_res = torch.matmul(X_q.to(torch.float32), W_q.to(torch.float32))\n",
                "        int8_res = int8_res * (x_scale * w_scale)\n",
                "    else:\n",
                "        int8_res = 0\n",
                "    \n",
                "    return int8_res + outlier_res\n",
                "\n",
                "# Test with synthetic data containing outliers\n",
                "X = torch.randn(16, 512)\n",
                "X[:, 7] *= 20.0 # Dimension 7 is an outlier\n",
                "W = torch.randn(512, 1024)\n",
                "\n",
                "start = time.time()\n",
                "fp32_res = torch.matmul(X, W)\n",
                "time_fp32 = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "int8_res = llm_int8_matmul(X, W)\n",
                "time_int8 = time.time() - start\n",
                "\n",
                "mse = torch.mean((fp32_res - int8_res)**2)\n",
                "print(f\"Reconstruction MSE (with outliers): {mse:.8f}\")\n",
                "print(f\"FP32 Time: {time_fp32:.4f}s\")\n",
                "print(f\"INT8 Simulation Time: {time_int8:.4f}s\")\n",
                "print(f\"\\nWeight storage effectively reduced to 8-bit (4x compression simulation)!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}