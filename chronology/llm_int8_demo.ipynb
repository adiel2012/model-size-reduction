{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§ª LLM.int8() From Scratch: Outlier-Aware Quantization (2022)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/llm_int8_demo.ipynb)\n",
    "\n",
    "## ğŸ“– The Theory: Vector-wise Quantization & Outliers\n",
    "\n",
    "Large Language Models (LLMs) exhibit a unique property: as they scale, certain feature dimensions in the activations develop **emergent outliers**. These outliers have magnitudes significantly larger than the rest of the values (often $>6.0$ while the mean is $<0.1$).\n",
    "\n",
    "### The Challenge\n",
    "Standard 8-bit quantization (INT8) maps a range $[min, max]$ to $[-128, 127]$. If a single outlier is $60.0$ and most values are $0.1$, the quantization scale becomes massive, crushing all the precision of the $0.1$ values into zero.\n",
    "\n",
    "### The LLM.int8() Solution\n",
    "LLM.int8() uses a two-part strategy:\n",
    "1.  **Vector-wise Quantization**: Each row/column of the weight matrix and activation matrix is quantized with its own scale factor.\n",
    "2.  **Outlier Handling**: Dimensions that contain outliers ($> \\text{threshold}$) are extracted and multiplied in high precision (**FP16/FP32**). The rest are multiplied in **INT8**.\n",
    "\n",
    "$$Y = X_{int8} \\cdot W_{int8} + X_{outlier} \\cdot W_{outlier}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. Load Model & Tokenizer\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).eval()\n",
    "\n",
    "def print_size_in_mb(tensor, name):\n",
    "    size = tensor.numel() * tensor.element_size() / (1024**2)\n",
    "    print(f\"{name} Size: {size:.2f} MB\")\n",
    "\n",
    "weight = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print_size_in_mb(weight, \"Original FP32 Weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Implementation: Manual Vector-wise Quantization\n",
    "Instead of using black-box libraries, we implement the core math manually to demonstrate the efficiency gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def llm_int8_matmul(X, W, threshold=6.0):\n",
    "    \"\"\"\n",
    "    A from-scratch simulation of outlier-aware matmul.\n",
    "    \"\"\"\n",
    "    # 1. Identify Outlier Dimensions\n",
    "    X = X.float()\n",
    "    W = W.float()\n",
    "    col_max = torch.max(torch.abs(X), dim=0)[0]\n",
    "    outlier_idx = torch.where(col_max > threshold)[0]\n",
    "    non_outlier_idx = torch.where(col_max <= threshold)[0]\n",
    "    \n",
    "    # 2. Extract and multiply Outliers in high precision\n",
    "    X_outlier = X[:, outlier_idx]\n",
    "    W_outlier = W[outlier_idx, :]\n",
    "    outlier_res = torch.matmul(X_outlier, W_outlier)\n",
    "    \n",
    "    # 3. Quantize and multiply others in INT8 simulation\n",
    "    X_non = X[:, non_outlier_idx]\n",
    "    W_non = W[non_outlier_idx, :]\n",
    "    \n",
    "    if X_non.shape[1] > 0:\n",
    "        # Vector-wise quant for X (row units)\n",
    "        x_scale = torch.max(torch.abs(X_non), dim=1, keepdim=True)[0] / 127\n",
    "        X_q = torch.round(X_non / (x_scale + 1e-8)).to(torch.int8)\n",
    "        \n",
    "        # Vector-wise quant for W (col units)\n",
    "        w_scale = torch.max(torch.abs(W_non), dim=0, keepdim=True)[0] / 127\n",
    "        W_q = torch.round(W_non / (w_scale + 1e-8)).to(torch.int8)\n",
    "        \n",
    "        # Matmul simulation (using Torch's matmul on float32 converted from int8)\n",
    "        int8_res = torch.matmul(X_q.to(torch.float32), W_q.to(torch.float32))\n",
    "        int8_res = int8_res * (x_scale * w_scale)\n",
    "    else:\n",
    "        int8_res = 0\n",
    "    \n",
    "    return int8_res + outlier_res\n",
    "\n",
    "# Test with synthetic data containing outliers\n",
    "X = torch.randn(16, 512)\n",
    "X[:, 7] *= 20.0 # Dimension 7 is an outlier\n",
    "W = torch.randn(512, 1024)\n",
    "\n",
    "start = time.time()\n",
    "fp32_res = torch.matmul(X, W)\n",
    "time_fp32 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "int8_res = llm_int8_matmul(X, W)\n",
    "time_int8 = time.time() - start\n",
    "\n",
    "mse = torch.mean((fp32_res - int8_res)**2)\n",
    "print(f\"Reconstruction MSE (with outliers): {mse:.8f}\")\n",
    "print(f\"FP32 Time: {time_fp32:.4f}s\")\n",
    "print(f\"INT8 Simulation Time: {time_int8:.4f}s\")\n",
    "print(f\"\\nWeight storage effectively reduced to 8-bit (4x compression simulation)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: why one outlier destroys naive INT8, and how LLM.int8() fixes it\nimport torch\n\nprint(\"=== LLM.int8(): Outlier problem on a 1Ã—6 activation vector ===\\n\")\n\n# 6-dim activation row: feature 3 is a massive outlier\nX_row = torch.tensor([[0.12, -0.09,  0.11, 52.3, -0.08,  0.14]])\nW_col = torch.tensor([[ 0.5], [-0.3], [ 0.7], [0.1], [-0.6], [ 0.4]])\n\nprint(f\"Activation X (1Ã—6): {X_row.tolist()[0]}\")\nprint(f\"  Feature 3 = 52.3  â† OUTLIER  (others â‰ˆ 0.1)\")\nprint(f\"Weight column W (6Ã—1): {[v[0] for v in W_col.tolist()]}\")\n\nfp32_result = (X_row @ W_col).item()\nprint(f\"\\nTrue FP32 result: {fp32_result:.4f}\")\n\n# â”€â”€ Naive INT8 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n--- Naive INT8 (global scale) ---\")\nx_scale = X_row.abs().max() / 127           # dominated by the outlier 52.3\nX_q     = (X_row / x_scale).round().clamp(-127, 127).to(torch.int8)\nprint(f\"Scale = {X_row.abs().max():.1f}/127 = {x_scale:.4f}\")\nprint(f\"INT8 X: {X_q.tolist()[0]}\")\nprint(f\"  â†’ Normal values (0.12) quantize to  {(0.12/x_scale.item()):.2f} â†’ 0  (precision destroyed!)\")\nw_scale      = W_col.abs().max() / 127\nW_q          = (W_col / w_scale).round().clamp(-127, 127).to(torch.int8)\nnaive_result = (X_q.float() @ W_q.float() * x_scale * w_scale).item()\nprint(f\"Naive INT8 result: {naive_result:.4f}   error = {abs(fp32_result - naive_result):.4f}\")\n\n# â”€â”€ LLM.int8() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n--- LLM.int8() (outlier separated) ---\")\nout_idx  = [3]           # outlier features  â†’ keep in FP32\nnorm_idx = [0,1,2,4,5]  # normal features   â†’ quantize INT8\n\n# FP32 path for the outlier\nfp_part = (X_row[:, out_idx] @ W_col[out_idx, :]).item()\n\n# INT8 path for normal features (vector-wise scale)\nX_n  = X_row[:, norm_idx]\nW_n  = W_col[norm_idx, :]\nxs   = X_n.abs().max() / 127\nws   = W_n.abs().max() / 127\nX_nq = (X_n / xs).round().clamp(-127, 127).to(torch.int8)\nW_nq = (W_n / ws).round().clamp(-127, 127).to(torch.int8)\nint8_part = (X_nq.float() @ W_nq.float() * xs * ws).item()\n\ncombined = fp_part + int8_part\nprint(f\"FP32 outlier path:  {X_row[0,3]:.1f} Ã— {W_col[3,0]:.1f} = {fp_part:.4f}\")\nprint(f\"INT8 normal path:   {int8_part:.4f}\")\nprint(f\"Combined:           {combined:.4f}   error = {abs(fp32_result - combined):.6f}\")\nprint(f\"\\nNaive INT8 error : {abs(fp32_result - naive_result):.4f}\")\nprint(f\"LLM.int8() error : {abs(fp32_result - combined):.6f}  ({abs(fp32_result-naive_result)/max(abs(fp32_result-combined),1e-9):.0f}Ã— better)\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}