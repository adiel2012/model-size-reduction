{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ LLM.int8(): Outlier-aware Weight Quantization (2022)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/llm_int8_demo.ipynb)\n",
    "\n",
    "LLM.int8() was one of the first methods to successfully quantize 175B+ parameter models to 8-bit without loss in perplexity. It achieves this by identifying \"outlier\" features in the activations and processing them in FP16 while using INT8 for everything else.\n",
    "\n",
    "In this notebook, we use PyTorch's `quantize_dynamic` to demonstrate the impact of 8-bit quantization on GPT-2 size and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize(\"temp.p\")/(1024*1024)\n",
    "    print(f'Size (MB): {size:.2f}')\n",
    "    os.remove('temp.p')\n",
    "\n",
    "# 1. Load Model & Tokenizer\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "model_fp32 = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "model_fp32.to('cpu').eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Performance Comparison (FP32 vs INT8)\n",
    "We use dynamic quantization to reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(\"--- Model Sizes ---\")\n",
    "print(\"Original FP32:\", end=\" \")\n",
    "print_size_of_model(model_fp32)\n",
    "print(\"Quantized INT8:\", end=\" \")\n",
    "print_size_of_model(model_int8)\n",
    "\n",
    "def benchmark_inference(model, input_text=\"Model quantization is\"):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        model.generate(**inputs, max_length=20)\n",
    "    return time.time() - start\n",
    "\n",
    "lat_32 = benchmark_inference(model_fp32)\n",
    "lat_8 = benchmark_inference(model_int8)\n",
    "print(f\"\\nFP32 Latency: {lat_32:.4f}s\")\n",
    "print(f\"INT8 Latency: {lat_8:.4f}s\")\n",
    "print(f\"Speedup: {lat_32/lat_8:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
