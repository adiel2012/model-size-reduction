{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ§ª LLM.int8() From Scratch: Outlier-Aware Quantization (2022)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/llm_int8_demo.ipynb)\n\n## ðŸ“– Theory: Vector-wise Quantization & Outliers\n\nLLM.int8() (Dettmers et al., 2022) was the first method to make 8-bit inference\npractical for models with **>6.7 billion parameters**, where a surprising\nphenomenon breaks naive INT8 quantization.\n\n### The Emergent Outlier Problem\n\nAs LLMs scale past ~6.7B parameters, a small fraction of hidden-state feature\ndimensions develop **systematic, massive magnitudes** -- values of order $10^1$-$10^2$\nwhile the rest of the tensor sits at $10^{-1}$-$10^{-2}$.\n\nThese *outlier dimensions* appear in the **same fixed set of channels** across all\ntokens and all layers. They are not random; they encode critical model structure.\n\n### Why Naive INT8 Fails\n\nStandard INT8 maps $[x_{\\min}, x_{\\max}]$ to $[-128, 127]$ with a single scale:\n\n$$\\text{scale} = \\frac{\\max|X|}{127}$$\n\nIf $\\max|X| = 52$, then scale $\\approx 0.41$, and a normal activation of $0.12$\nquantizes to $\\text{round}(0.12/0.41) = 0$ -- **total precision loss** for\nthe majority of features.\n\n### The Mixed-Precision Decomposition\n\n$$Y = X_{\\text{int8}} W_{\\text{int8}} + X_{\\text{out}} W_{\\text{out}}$$\n\n1. **Identify outlier columns** in $X$: those where $\\max|X_{:,j}| > \\theta$ (typically $\\theta = 6$).\n2. **Outlier path** (~1% of features): multiply in **FP16** -- exact, no rounding error.\n3. **Normal path** (~99% of features): quantize with **vector-wise INT8** scales.\n\n### Vector-wise vs Tensor-wise Quantization\n\n| | Tensor-wise | Vector-wise |\n|---|---|---|\n| Scale granularity | One scale for the whole matrix | One scale per row of $X$, per column of $W$ |\n| Precision loss | High (outliers dominate scale) | Low (each scale fits its row/column) |\n| Memory overhead | Negligible | One float32 per row/column |\n\nVector-wise scales reduce INT8 quantization error by 3-10x on normal features.\n\n### Threshold Selection\n\nThe outlier threshold $\\theta = 6.0$ was chosen empirically:\n* Below 6.7B params: no systematic outliers -- standard INT8 suffices.\n* Above 6.7B params: outlier dimensions emerge -- $\\theta = 6$ cleanly separates them.\n\n### Memory Savings\n\nFor a 175B-parameter model:\n* FP16 baseline: $175 \\times 10^9 \\times 2 \\approx 350$ GB\n* LLM.int8(): $175 \\times 10^9 \\times 1 \\approx 175$ GB (~**2x reduction**)\n\nThis reduced 175B inference from 5-8 A100s down to 2-3 A100s.\n\n### Limitations\n* INT8 arithmetic is slower than FP16 on some GPU architectures.\n* Mixed-precision split adds implementation complexity.\n* Designed for 8-bit only -- accuracy degrades at 4-bit.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. Load Model & Tokenizer\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).eval()\n",
    "\n",
    "def print_size_in_mb(tensor, name):\n",
    "    size = tensor.numel() * tensor.element_size() / (1024**2)\n",
    "    print(f\"{name} Size: {size:.2f} MB\")\n",
    "\n",
    "weight = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print_size_in_mb(weight, \"Original FP32 Weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Implementation: Manual Vector-wise Quantization\n",
    "Instead of using black-box libraries, we implement the core math manually to demonstrate the efficiency gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def llm_int8_matmul(X, W, threshold=6.0):\n",
    "    \"\"\"\n",
    "    A from-scratch simulation of outlier-aware matmul.\n",
    "    \"\"\"\n",
    "    # 1. Identify Outlier Dimensions\n",
    "    X = X.float()\n",
    "    W = W.float()\n",
    "    col_max = torch.max(torch.abs(X), dim=0)[0]\n",
    "    outlier_idx = torch.where(col_max > threshold)[0]\n",
    "    non_outlier_idx = torch.where(col_max <= threshold)[0]\n",
    "    \n",
    "    # 2. Extract and multiply Outliers in high precision\n",
    "    X_outlier = X[:, outlier_idx]\n",
    "    W_outlier = W[outlier_idx, :]\n",
    "    outlier_res = torch.matmul(X_outlier, W_outlier)\n",
    "    \n",
    "    # 3. Quantize and multiply others in INT8 simulation\n",
    "    X_non = X[:, non_outlier_idx]\n",
    "    W_non = W[non_outlier_idx, :]\n",
    "    \n",
    "    if X_non.shape[1] > 0:\n",
    "        # Vector-wise quant for X (row units)\n",
    "        x_scale = torch.max(torch.abs(X_non), dim=1, keepdim=True)[0] / 127\n",
    "        X_q = torch.round(X_non / (x_scale + 1e-8)).to(torch.int8)\n",
    "        \n",
    "        # Vector-wise quant for W (col units)\n",
    "        w_scale = torch.max(torch.abs(W_non), dim=0, keepdim=True)[0] / 127\n",
    "        W_q = torch.round(W_non / (w_scale + 1e-8)).to(torch.int8)\n",
    "        \n",
    "        # Matmul simulation (using Torch's matmul on float32 converted from int8)\n",
    "        int8_res = torch.matmul(X_q.to(torch.float32), W_q.to(torch.float32))\n",
    "        int8_res = int8_res * (x_scale * w_scale)\n",
    "    else:\n",
    "        int8_res = 0\n",
    "    \n",
    "    return int8_res + outlier_res\n",
    "\n",
    "# Test with synthetic data containing outliers\n",
    "X = torch.randn(16, 512)\n",
    "X[:, 7] *= 20.0 # Dimension 7 is an outlier\n",
    "W = torch.randn(512, 1024)\n",
    "\n",
    "start = time.time()\n",
    "fp32_res = torch.matmul(X, W)\n",
    "time_fp32 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "int8_res = llm_int8_matmul(X, W)\n",
    "time_int8 = time.time() - start\n",
    "\n",
    "mse = torch.mean((fp32_res - int8_res)**2)\n",
    "print(f\"Reconstruction MSE (with outliers): {mse:.8f}\")\n",
    "print(f\"FP32 Time: {time_fp32:.4f}s\")\n",
    "print(f\"INT8 Simulation Time: {time_int8:.4f}s\")\n",
    "print(f\"\\nWeight storage effectively reduced to 8-bit (4x compression simulation)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nprint(\"=== LLM.int8(): Outlier problem on a 1Ã—6 activation vector ===\\n\")\n\n# 6-dim activation row: feature 3 is a massive outlier\nX_row = torch.tensor([[0.12, -0.09,  0.11, 52.3, -0.08,  0.14]])  # shape [1, 6]\nW_col = torch.tensor([[ 0.5], [-0.3], [ 0.7], [0.1], [-0.6], [ 0.4]])  # shape [6, 1]\n\nprint(f\"Activation X (1Ã—6): {X_row.tolist()[0]}\")\nprint(f\"  Feature 3 = 52.3  â† OUTLIER  (others â‰ˆ 0.1)\")\nprint(f\"Weight column W (6Ã—1): {[v[0] for v in W_col.tolist()]}\")\n\n# True dot product\n# 0.12Ã—0.5 + (âˆ’0.09)Ã—(âˆ’0.3) + 0.11Ã—0.7 + 52.3Ã—0.1 + (âˆ’0.08)Ã—(âˆ’0.6) + 0.14Ã—0.4\n# = 0.060 + 0.027 + 0.077 + 5.230 + 0.048 + 0.056\nfp32_result = (X_row @ W_col).item()  # => 5.4980\nprint(f\"\\nTrue FP32 result: {fp32_result:.4f}\")\n\n# â”€â”€ Naive INT8 (global scale) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n--- Naive INT8 (global scale) ---\")\n\n# Scale is dominated by the outlier 52.3 â†’ normal values lose all precision\nx_scale = X_row.abs().max() / 127          # => 52.3 / 127 = 0.4118\nX_q = (X_row / x_scale).round().clamp(-127, 127).to(torch.int8)\n# Each value Ã· 0.4118:  [0.29â†’0, -0.22â†’0, 0.27â†’0, 127.0â†’127, -0.19â†’0, 0.34â†’0]\n# X_q                                                => [0, 0, 0, 127, 0, 0]\n\nprint(f\"Scale = {X_row.abs().max():.1f}/127 = {x_scale:.4f}\")\nprint(f\"INT8 X: {X_q.tolist()[0]}\")\nprint(f\"  â†’ Normal values (0.12) quantize to  {(0.12/x_scale.item()):.2f} â†’ 0  (precision destroyed!)\")\n\nw_scale = W_col.abs().max() / 127          # => 0.7 / 127 = 0.0055\nW_q = (W_col / w_scale).round().clamp(-127, 127).to(torch.int8)\n# Each value Ã· 0.0055: [90.7â†’91, -54.4â†’-54, 127.0â†’127, 18.1â†’18, -108.9â†’-109, 72.6â†’73]\n# W_q                                                => [[91], [-54], [127], [18], [-109], [73]]\n\n# Dot product in int8 space: only the outlier term survives\n# [0,0,0,127,0,0] Â· [91,-54,127,18,-109,73] = 127Ã—18 = 2286\n# Rescale: 2286 Ã— 0.4118 Ã— 0.0055\nnaive_result = (X_q.float() @ W_q.float() * x_scale * w_scale).item()  # => 5.1887\nprint(f\"Naive INT8 result: {naive_result:.4f}   error = {abs(fp32_result - naive_result):.4f}\")\n# error => 0.3093  (the 5 normal features contributed nothing)\n\n# â”€â”€ LLM.int8(): separate outlier â†’ FP32, normal â†’ INT8 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n--- LLM.int8() (outlier separated) ---\")\n\nout_idx  = [3]           # outlier features â†’ FP32 path\nnorm_idx = [0,1,2,4,5]  # normal features  â†’ INT8 path\n\n# FP32 path: exact multiplication for the outlier\nfp_part = (X_row[:, out_idx] @ W_col[out_idx, :]).item()  # => 52.3 Ã— 0.1 = 5.2300\n\n# INT8 path for normal features (vector-wise scale)\nX_n = X_row[:, norm_idx]    # => [[0.12, -0.09, 0.11, -0.08, 0.14]]\nW_n = W_col[norm_idx, :]    # => [[0.5], [-0.3], [0.7], [-0.6], [0.4]]\n\nxs = X_n.abs().max() / 127  # => 0.14 / 127 = 0.0011   (small: fits normal values well)\nws = W_n.abs().max() / 127  # => 0.70 / 127 = 0.0055\n\nX_nq = (X_n / xs).round().clamp(-127, 127).to(torch.int8)\n# [0.12/0.0011â†’109, -0.09/0.0011â†’-82, 0.11/0.0011â†’100, -0.08/0.0011â†’-73, 0.14/0.0011â†’127]\n# X_nq                                               => [109, -82, 100, -73, 127]\n\nW_nq = (W_n / ws).round().clamp(-127, 127).to(torch.int8)\n# [0.5/0.0055â†’91, -0.3/0.0055â†’-54, 0.7/0.0055â†’127, -0.6/0.0055â†’-109, 0.4/0.0055â†’73]\n# W_nq                                               => [[91], [-54], [127], [-109], [73]]\n\n# INT8 dot product then rescale:\n# 109Ã—91 + (-82)Ã—(-54) + 100Ã—127 + (-73)Ã—(-109) + 127Ã—73\n# = 9919 + 4428 + 12700 + 7957 + 9271 = 44275\n# Ã— 0.0011 Ã— 0.0055\nint8_part = (X_nq.float() @ W_nq.float() * xs * ws).item()  # => 44275 Ã— xs Ã— ws â‰ˆ 0.2690\n\ncombined = fp_part + int8_part  # => 5.2300 + 0.2690 = 5.4990\n\nprint(f\"FP32 outlier path:  {X_row[0,3]:.1f} Ã— {W_col[3,0]:.1f} = {fp_part:.4f}\")\n# => 52.3 Ã— 0.1 = 5.2300\nprint(f\"INT8 normal path:   {int8_part:.4f}\")\n# => â‰ˆ 0.2690\nprint(f\"Combined:           {combined:.4f}   error = {abs(fp32_result - combined):.6f}\")\n# => 5.4990   error â‰ˆ 0.001000\n\nprint(f\"\\nNaive INT8 error : {abs(fp32_result - naive_result):.4f}\")   # => 0.3093\nprint(f\"LLM.int8() error : {abs(fp32_result - combined):.6f}  ({abs(fp32_result-naive_result)/max(abs(fp32_result-combined),1e-9):.0f}Ã— better)\")\n# => ~300Ã— better  (the outlier is handled exactly; normals fit the INT8 range perfectly)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ“š References\n\n1. **Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L.** (2022).  \n   *LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.* NeurIPS 2022.  \n   [arXiv:2208.07339](https://arxiv.org/abs/2208.07339)\n\n2. **Dettmers, T., Svirschevski, R., Egiazarian, V., et al.** (2023).  \n   *SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression.*  \n   [arXiv:2306.03078](https://arxiv.org/abs/2306.03078)\n\n3. **Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., & Han, S.** (2022).  \n   *SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.* ICML 2023.  \n   [arXiv:2211.10438](https://arxiv.org/abs/2211.10438)\n\n4. **Zafrir, O., Boudoukh, G., Izsak, P., & Wasserblat, M.** (2019).  \n   *Q8BERT: Quantized 8Bit BERT.*  \n   [arXiv:1910.06188](https://arxiv.org/abs/1910.06188)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply vector-wise INT8 quantization to all 2D weight matrices of GPT-2 and compare perplexity before and after."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\n# Apply vector-wise INT8 quantization to all 2D weight matrices\n# (simulates the LLM.int8() weight path without outlier separation for a global view)\nmodel_q = copy.deepcopy(model)\nfor name, param in model_q.named_parameters():\n    if param.dim() == 2:\n        W = param.data.float()\n        # Vector-wise scale: one scale per output row\n        row_scale = W.abs().max(dim=1, keepdim=True)[0] / 127 + 1e-8\n        W_q = (W / row_scale).round().clamp(-127, 127) * row_scale\n        param.data = W_q.to(param.dtype)\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"LLM.int8() GPT-2 Perplexity: {quant_ppl:.2f}\")\nprint(f\"Delta:                        {quant_ppl - baseline_ppl:+.2f}\")\n"
  },
  {
   "cell_type": "code",
   "source": "import torch, torch.nn as nn, copy\n\ntry:\n    from transformers.pytorch_utils import Conv1D\nexcept ImportError:\n    from transformers.modeling_utils import Conv1D\n\n# â”€â”€ Full LLM.int8(): outlier-aware mixed-precision decomposition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Replaces every Conv1D layer with a module that:\n#   â€¢ stores W in INT8 with per-input-row scale\n#   â€¢ at forward time detects outlier activation columns (|col_max| > Î¸)\n#   â€¢ routes outlier dims â†’ FP32 matmul  (exact, no rounding error)\n#   â€¢ routes normal  dims â†’ INT8-simulated matmul (quantised activations + INT8 weight)\n\nclass OutlierAwareLinear(nn.Module):\n    \"\"\"\n    Drop-in for GPT-2's Conv1D.\n    Weight stored as INT8 (row-wise scale).\n    Outlier input dimensions â†’ FP32 path; normal dims â†’ INT8 path.\n    \"\"\"\n    def __init__(self, conv1d: Conv1D, threshold: float = 6.0):\n        super().__init__()\n        self.threshold = threshold\n        W = conv1d.weight.data.float()                                    # [in_features, out_features]\n        row_scale = W.abs().max(dim=1, keepdim=True)[0] / 127 + 1e-8    # one scale per input row\n        self.register_buffer(\"W_int8\",    (W / row_scale).round().clamp(-127, 127).to(torch.int8))\n        self.register_buffer(\"row_scale\", row_scale)\n        bias = conv1d.bias\n        self.register_buffer(\"bias_buf\",  bias.data.float() if bias is not None else None)\n\n    def forward(self, x):\n        orig_shape = x.shape\n        x_flat = x.reshape(-1, x.shape[-1]).float()           # [N, in_features]\n        W_fp   = self.W_int8.float() * self.row_scale         # dequantize weights: [in, out]\n\n        # Detect outlier input-feature columns across the current batch\n        col_max   = x_flat.abs().max(dim=0)[0]                # max |activation| per feature\n        out_mask  = col_max > self.threshold                   # True â†’ outlier feature\n        norm_mask = ~out_mask\n\n        # FP32 path: exact multiply for high-magnitude features\n        fp_part = x_flat[:, out_mask] @ W_fp[out_mask] if out_mask.any() else 0.0\n\n        # INT8-simulated path: quantise activations, use INT8 weights\n        if norm_mask.any():\n            X_n     = x_flat[:, norm_mask]                    # [N, n_normal]\n            W_n_fp  = W_fp[norm_mask]                         # dequantized weight slice\n            # Vector-wise activation scale (one per row of X_n)\n            x_scale = X_n.abs().max(dim=1, keepdim=True)[0] / 127 + 1e-8\n            X_nq    = (X_n / x_scale).round().clamp(-127, 127) * x_scale   # quantizeâ†’dequantize\n            int8_part = X_nq @ W_n_fp\n        else:\n            int8_part = 0.0\n\n        out = fp_part + int8_part\n        if self.bias_buf is not None:\n            out = out + self.bias_buf\n        return out.reshape(*orig_shape[:-1], -1).to(x.dtype)\n\n\ndef replace_with_outlier_aware(mdl, threshold=6.0):\n    \"\"\"Collect all Conv1D modules then replace them (safe: no mid-iteration mutation).\"\"\"\n    replacements = []\n    for parent in mdl.modules():\n        for child_name, child in parent.named_children():\n            if isinstance(child, Conv1D):\n                replacements.append((parent, child_name, child))\n    for parent, child_name, child in replacements:\n        setattr(parent, child_name, OutlierAwareLinear(child, threshold=threshold))\n    return mdl\n\n\nmodel_q2 = replace_with_outlier_aware(copy.deepcopy(model), threshold=6.0)\n\n# Show how many outlier dimensions appear in the first layer's input\nwith torch.no_grad():\n    emb = model.transformer.wte(inputs[\"input_ids\"])          # [1, seq, 768]\n    col_max = emb.reshape(-1, emb.shape[-1]).abs().max(dim=0)[0]\n    n_out = (col_max > 6.0).sum().item()\n    print(f\"Outlier dims in 1st-layer input (Î¸=6.0): {n_out} / {emb.shape[-1]}\")\n    print(\"(GPT-2 is small â€” systematic outliers only emerge above ~6.7B params)\\n\")\n\noutlier_ppl = perplexity(model_q2, inputs)\n\n# â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(f\"{'Method':<40} {'PPL':>6}  {'Î” PPL':>7}\")\nprint(\"-\" * 56)\nprint(f\"{'Baseline (FP32)':<40} {baseline_ppl:>6.2f}  {'â€”':>7}\")\nprint(f\"{'Vector-wise INT8 (no outlier sep.)':<40} {quant_ppl:>6.2f}  {quant_ppl - baseline_ppl:>+7.2f}\")\nprint(f\"{'Full LLM.int8() (outlier-aware)':<40} {outlier_ppl:>6.2f}  {outlier_ppl - baseline_ppl:>+7.2f}\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}