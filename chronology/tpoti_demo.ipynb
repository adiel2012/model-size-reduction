{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ§ª T-Poti From Scratch: Ultra-Low Precision (2026)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/tpoti_demo.ipynb)\n\n## ðŸ“– Theory: Binary and Quaternary Frontiers\n\n> **Note**: T-Poti is an illustrative projection of the 2026 research frontier,\n> synthesising real techniques from the binary/ternary quantization literature.\n\nT-Poti pushes to **1-bit (binary)** and **2-bit (quaternary)** precision without\nretraining -- targeting extreme edge devices (microcontrollers, wearables).\n\n### 1-bit: Optimal Binary Quantization\n\nWith only two levels $\\{-\\alpha, +\\alpha\\}$, find the scalar $\\alpha$ minimising:\n\n$$\\min_{\\alpha,\\,B \\in \\{-1,+1\\}^n} \\|W - \\alpha B\\|_2^2$$\n\nHolding $B = \\text{sign}(W)$ fixed, the optimal scale has a closed form:\n\n$$\\alpha^* = \\frac{\\|W\\|_1}{n} = \\text{mean}(|W|)$$\n\nThis result follows by differentiating and setting the derivative to zero.\nIt is the same closed-form used by XNOR-Net and BitNet.\n\n### 2-bit: Quaternary Levels\n\nWith four levels $\\{-1,\\,-1/3,\\,+1/3,\\,+1\\}$ (scaled by $\\max|W|$),\neach weight is encoded in 2 bits. The level set is chosen to minimise expected\nsquared error for a uniform distribution over $[-1,1]$.\n\n### Error Compensation via Multi-Codebook\n\nAt extreme precision the rounding error is large. T-Poti-style methods use\n**iterative residual quantization**:\n1. Quantize $W$ with binary mask $B_1$: $\\hat{W}_1 = \\alpha_1 B_1$.\n2. Compute residual $E = W - \\hat{W}_1$.\n3. Quantize $E$ with a second binary mask $B_2$: $\\hat{W}_2 = \\alpha_2 B_2$.\n4. Final representation: $W \\approx \\alpha_1 B_1 + \\alpha_2 B_2$ (2-bit effective).\n\nThis multi-codebook strategy is also used in **Residual Vector Quantization (RVQ)**.\n\n### Information-Theoretic Lower Bound\n\nFor $W \\sim \\mathcal{N}(0,1)$, the minimum achievable MSE at $b$ bits per weight:\n\n$$\\text{MSE}_{\\min} \\approx \\frac{\\pi\\sqrt{3}}{2} \\cdot 2^{-2b}$$\n\n| Bits | Min MSE |\n|---|---|\n| 1-bit | ~0.363 |\n| 2-bit | ~0.091 |\n| 4-bit (NF4) | ~0.006 |\n\n### Limitations\n* Very high quantization error -- mostly viable for QAT models.\n* 2-bit PTQ on large pre-trained models degrades accuracy significantly\n  without careful calibration and error-compensation techniques.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def tpoti_1bit_quantize(W):\n",
    "    \"\"\"\n",
    "    Manual implementation of Optimal Binary Quantization (1-bit).\n",
    "    Maps W -> alpha * sign(W)\n",
    "    \"\"\"\n",
    "    # 1. Optimal Alpha calculation for a binary mask\n",
    "    # Formally: alpha = mean(abs(W))\n",
    "    alpha = torch.mean(torch.abs(W))\n",
    "    \n",
    "    # 2. Map to Binary {-1, 1}\n",
    "    B = torch.sign(W)\n",
    "    B[B == 0] = 1 # Handle zeros\n",
    "    \n",
    "    return B, alpha\n",
    "\n",
    "def tpoti_2bit_quantize(W):\n",
    "    \"\"\"\n",
    "    Manual 2-bit (Quaternary) quantization.\n",
    "    Levels: {-1, -0.33, 0.33, 1} scaled by alpha.\n",
    "    \"\"\"\n",
    "    # Normalize to [-1, 1]\n",
    "    abs_max = torch.max(torch.abs(W))\n",
    "    W_norm = W / abs_max\n",
    "    \n",
    "    # 2-bit levels\n",
    "    levels = torch.tensor([-1.0, -0.33, 0.33, 1.0])\n",
    "    \n",
    "    # Map each value to closest level\n",
    "    W_flat = W_norm.view(-1, 1)\n",
    "    diff = torch.abs(W_flat - levels.view(1, -1))\n",
    "    indices = torch.argmin(diff, dim=1)\n",
    "    \n",
    "    W_q = levels[indices].view(W.shape)\n",
    "    return W_q, abs_max\n",
    "\n",
    "# Demonstration\n",
    "W = torch.randn(1024, 1024)\n",
    "B, alpha = tpoti_1bit_quantize(W)\n",
    "W_target_2b, scale_2b = tpoti_2bit_quantize(W)\n",
    "\n",
    "print(f\"--- 1-bit Result ---\")\n",
    "print(f\"Binary Mask Sample: {B[0, :5].tolist()}\")\n",
    "print(f\"Optimal Alpha: {alpha:.4f}\")\n",
    "print(f\"1-bit MSE: {torch.mean((W - B*alpha)**2):.6f}\")\n",
    "\n",
    "print(f\"\\n--- 2-bit Result ---\")\n",
    "print(f\"Quaternary Sample: {W_target_2b[0, :5].tolist()}\")\n",
    "print(f\"2-bit MSE: {torch.mean((W - W_target_2b*scale_2b)**2):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: 1-bit and 2-bit quantization on a small vector\nimport torch\n\nw = torch.tensor([0.7, -0.4, 0.9, -0.1, 0.3, -0.8])\nprint(f\"Original weights: {[round(v,2) for v in w.tolist()]}\")\n\n# â”€â”€ 1-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n=== 1-bit  (Binary: {-1, +1}) ===\")\nalpha = w.abs().mean()\nB     = w.sign()\nB[B == 0] = 1                         # no exact zeros allowed\nrecon_1b = B * alpha\n\nprint(f\"Optimal alpha = mean|w| = {alpha:.4f}\")\nprint(f\"  {'orig':>7}  {'sign':>6}  {'recon':>8}  {'err':>8}\")\nfor orig, b, r in zip(w.tolist(), B.tolist(), recon_1b.tolist()):\n    print(f\"  {orig:+7.4f}  {b:+6.0f}  {r:+8.4f}  {abs(orig-r):8.4f}\")\nprint(f\"1-bit MSE: {((w - recon_1b)**2).mean():.6f}\")\n\n# â”€â”€ 2-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n=== 2-bit  (Quaternary: {-1, -0.33, +0.33, +1}) ===\")\nlevels  = torch.tensor([-1.0, -0.33, 0.33, 1.0])\nabs_max = w.abs().max()\nw_norm  = w / abs_max\ndiff    = (w_norm.view(-1, 1) - levels.view(1, -1)).abs()\nindices = diff.argmin(dim=1)\nw_q2    = levels[indices]\nrecon_2b = w_q2 * abs_max\n\nprint(f\"Scale (abs_max) = {abs_max:.4f}\")\nprint(f\"  {'orig':>7}  {'norm':>7}  {'level':>7}  {'recon':>8}  {'err':>8}\")\nfor orig, nrm, lv, r in zip(w.tolist(), w_norm.tolist(), w_q2.tolist(), recon_2b.tolist()):\n    print(f\"  {orig:+7.4f}  {nrm:+7.4f}  {lv:+7.2f}  {r:+8.4f}  {abs(orig-r):8.4f}\")\nprint(f\"2-bit MSE: {((w - recon_2b)**2).mean():.6f}\")\n\nprint(f\"\\nSummary: 2-bit MSE ({((w-recon_2b)**2).mean():.4f}) < 1-bit MSE ({((w-recon_1b)**2).mean():.4f})  as expected\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity:       {baseline_ppl:.2f}\")\n\n# 1-bit\nmodel_1b = copy.deepcopy(model)\nfor name, param in model_1b.named_parameters():\n    if param.dim() == 2:\n        B, alpha = tpoti_1bit_quantize(param.data)\n        param.data = B.float() * alpha\nppl_1b = perplexity(model_1b, inputs)\nprint(f\"T-Poti 1-bit GPT-2 Perplexity:   {ppl_1b:.2f}\")\nprint(f\"Delta:                           {ppl_1b - baseline_ppl:+.2f}\")\n\n# 2-bit\nmodel_2b = copy.deepcopy(model)\nfor name, param in model_2b.named_parameters():\n    if param.dim() == 2:\n        W_q, scale = tpoti_2bit_quantize(param.data)\n        param.data = W_q * scale\nppl_2b = perplexity(model_2b, inputs)\nprint(f\"T-Poti 2-bit GPT-2 Perplexity:   {ppl_2b:.2f}\")\nprint(f\"Delta:                           {ppl_2b - baseline_ppl:+.2f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ“š References\n\n1. **Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., & Bengio, Y.** (2016).  \n   *Binarized Neural Networks.* NeurIPS 2016.  \n   [arXiv:1602.02830](https://arxiv.org/abs/1602.02830)\n\n2. **Rastegari, M., Ordonez, V., Redmon, J., & Farhadi, A.** (2016).  \n   *XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.* ECCV 2016.  \n   [arXiv:1603.05279](https://arxiv.org/abs/1603.05279)\n\n3. **Ma, S., Wang, H., Ma, L., et al.** (2024).  \n   *The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits.*  \n   [arXiv:2402.17764](https://arxiv.org/abs/2402.17764)\n\n4. **Zeghidour, N., Luebs, A., Omran, A., Skerry-Ryan, R., & Tagliasacchi, M.** (2021).  \n   *SoundStream: An End-to-End Neural Audio Codec.* (Residual Vector Quantization)  \n   [arXiv:2107.03312](https://arxiv.org/abs/2107.03312)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}