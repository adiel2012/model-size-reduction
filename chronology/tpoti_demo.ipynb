{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª T-Poti From Scratch: Ultra-Low Precision (2026)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/tpoti_demo.ipynb)\n",
    "\n",
    "## ðŸ“– The Theory: Binary and Quaternary Frontiers\n",
    "\n",
    "T-Poti represents the bleeding edge of quantization research for 2026. As models grow, even 4-bit INT4 is sometimes too large for extreme edge devices. T-Poti focuses on **1-bit (Binary)** and **2-bit (Quaternary)** precision without requiring extensive retraining.\n",
    "\n",
    "### The Difficulty of 1-bit\n",
    "When you only have two levels (e.g., `-1` and `1`), the \"quantization error\" is massive. Standard rounding fails completely. T-Poti use a combination of:\n",
    "1.  **Optimal Scaling**: Finding the scalar $\\alpha$ that minimizes $||W - \\alpha \\cdot sign(W)||^2$. \n",
    "2.  **Error Compensation**: Iterative refinement of the binary mask to preserve the most important activation directions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "def tpoti_1bit_quantize(W):\n",
    "    \"\"\"\n",
    "    Manual implementation of Optimal Binary Quantization (1-bit).\n",
    "    Maps W -> alpha * sign(W)\n",
    "    \"\"\"\n",
    "    # 1. Optimal Alpha calculation for a binary mask\n",
    "    # Formally: alpha = mean(abs(W))\n",
    "    alpha = torch.mean(torch.abs(W))\n",
    "    \n",
    "    # 2. Map to Binary {-1, 1}\n",
    "    B = torch.sign(W)\n",
    "    B[B == 0] = 1 # Handle zeros\n",
    "    \n",
    "    return B, alpha\n",
    "\n",
    "def tpoti_2bit_quantize(W):\n",
    "    \"\"\"\n",
    "    Manual 2-bit (Quaternary) quantization.\n",
    "    Levels: {-1, -0.33, 0.33, 1} scaled by alpha.\n",
    "    \"\"\"\n",
    "    # Normalize to [-1, 1]\n",
    "    abs_max = torch.max(torch.abs(W))\n",
    "    W_norm = W / abs_max\n",
    "    \n",
    "    # 2-bit levels\n",
    "    levels = torch.tensor([-1.0, -0.33, 0.33, 1.0])\n",
    "    \n",
    "    # Map each value to closest level\n",
    "    W_flat = W_norm.view(-1, 1)\n",
    "    diff = torch.abs(W_flat - levels.view(1, -1))\n",
    "    indices = torch.argmin(diff, dim=1)\n",
    "    \n",
    "    W_q = levels[indices].view(W.shape)\n",
    "    return W_q, abs_max\n",
    "\n",
    "# Demonstration\n",
    "W = torch.randn(1024, 1024)\n",
    "B, alpha = tpoti_1bit_quantize(W)\n",
    "W_target_2b, scale_2b = tpoti_2bit_quantize(W)\n",
    "\n",
    "print(f\"--- 1-bit Result ---\")\n",
    "print(f\"Binary Mask Sample: {B[0, :5].tolist()}\")\n",
    "print(f\"Optimal Alpha: {alpha:.4f}\")\n",
    "print(f\"1-bit MSE: {torch.mean((W - B*alpha)**2):.6f}\")\n",
    "\n",
    "print(f\"\\n--- 2-bit Result ---\")\n",
    "print(f\"Quaternary Sample: {W_target_2b[0, :5].tolist()}\")\n",
    "print(f\"2-bit MSE: {torch.mean((W - W_target_2b*scale_2b)**2):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: 1-bit and 2-bit quantization on a small vector\nimport torch\n\nw = torch.tensor([0.7, -0.4, 0.9, -0.1, 0.3, -0.8])\nprint(f\"Original weights: {[round(v,2) for v in w.tolist()]}\")\n\n# â”€â”€ 1-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n=== 1-bit  (Binary: {-1, +1}) ===\")\nalpha = w.abs().mean()\nB     = w.sign()\nB[B == 0] = 1                         # no exact zeros allowed\nrecon_1b = B * alpha\n\nprint(f\"Optimal alpha = mean|w| = {alpha:.4f}\")\nprint(f\"  {'orig':>7}  {'sign':>6}  {'recon':>8}  {'err':>8}\")\nfor orig, b, r in zip(w.tolist(), B.tolist(), recon_1b.tolist()):\n    print(f\"  {orig:+7.4f}  {b:+6.0f}  {r:+8.4f}  {abs(orig-r):8.4f}\")\nprint(f\"1-bit MSE: {((w - recon_1b)**2).mean():.6f}\")\n\n# â”€â”€ 2-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n=== 2-bit  (Quaternary: {-1, -0.33, +0.33, +1}) ===\")\nlevels  = torch.tensor([-1.0, -0.33, 0.33, 1.0])\nabs_max = w.abs().max()\nw_norm  = w / abs_max\ndiff    = (w_norm.view(-1, 1) - levels.view(1, -1)).abs()\nindices = diff.argmin(dim=1)\nw_q2    = levels[indices]\nrecon_2b = w_q2 * abs_max\n\nprint(f\"Scale (abs_max) = {abs_max:.4f}\")\nprint(f\"  {'orig':>7}  {'norm':>7}  {'level':>7}  {'recon':>8}  {'err':>8}\")\nfor orig, nrm, lv, r in zip(w.tolist(), w_norm.tolist(), w_q2.tolist(), recon_2b.tolist()):\n    print(f\"  {orig:+7.4f}  {nrm:+7.4f}  {lv:+7.2f}  {r:+8.4f}  {abs(orig-r):8.4f}\")\nprint(f\"2-bit MSE: {((w - recon_2b)**2).mean():.6f}\")\n\nprint(f\"\\nSummary: 2-bit MSE ({((w-recon_2b)**2).mean():.4f}) < 1-bit MSE ({((w-recon_1b)**2).mean():.4f})  as expected\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity:       {baseline_ppl:.2f}\")\n\n# 1-bit\nmodel_1b = copy.deepcopy(model)\nfor name, param in model_1b.named_parameters():\n    if param.dim() == 2:\n        B, alpha = tpoti_1bit_quantize(param.data)\n        param.data = B.float() * alpha\nppl_1b = perplexity(model_1b, inputs)\nprint(f\"T-Poti 1-bit GPT-2 Perplexity:   {ppl_1b:.2f}\")\nprint(f\"Delta:                           {ppl_1b - baseline_ppl:+.2f}\")\n\n# 2-bit\nmodel_2b = copy.deepcopy(model)\nfor name, param in model_2b.named_parameters():\n    if param.dim() == 2:\n        W_q, scale = tpoti_2bit_quantize(param.data)\n        param.data = W_q * scale\nppl_2b = perplexity(model_2b, inputs)\nprint(f\"T-Poti 2-bit GPT-2 Perplexity:   {ppl_2b:.2f}\")\nprint(f\"Delta:                           {ppl_2b - baseline_ppl:+.2f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}