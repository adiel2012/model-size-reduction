{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª T-Poti From Scratch: Ultra-Low Precision (2026)\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/tpoti_demo.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: Binary and Quaternary Frontiers\n",
                "\n",
                "T-Poti represents the bleeding edge of quantization research for 2026. As models grow, even 4-bit INT4 is sometimes too large for extreme edge devices. T-Poti focuses on **1-bit (Binary)** and **2-bit (Quaternary)** precision without requiring extensive retraining.\n",
                "\n",
                "### The Difficulty of 1-bit\n",
                "When you only have two levels (e.g., `-1` and `1`), the \"quantization error\" is massive. Standard rounding fails completely. T-Poti use a combination of:\n",
                "1.  **Optimal Scaling**: Finding the scalar $\\alpha$ that minimizes $||W - \\alpha \\cdot sign(W)||^2$. \n",
                "2.  **Error Compensation**: Iterative refinement of the binary mask to preserve the most important activation directions.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "\n",
                "def tpoti_1bit_quantize(W):\n",
                "    \"\"\"\n",
                "    Manual implementation of Optimal Binary Quantization (1-bit).\n",
                "    Maps W -> alpha * sign(W)\n",
                "    \"\"\"\n",
                "    # 1. Optimal Alpha calculation for a binary mask\n",
                "    # Formally: alpha = mean(abs(W))\n",
                "    alpha = torch.mean(torch.abs(W))\n",
                "    \n",
                "    # 2. Map to Binary {-1, 1}\n",
                "    B = torch.sign(W)\n",
                "    B[B == 0] = 1 # Handle zeros\n",
                "    \n",
                "    return B, alpha\n",
                "\n",
                "def tpoti_2bit_quantize(W):\n",
                "    \"\"\"\n",
                "    Manual 2-bit (Quaternary) quantization.\n",
                "    Levels: {-1, -0.33, 0.33, 1} scaled by alpha.\n",
                "    \"\"\"\n",
                "    # Normalize to [-1, 1]\n",
                "    abs_max = torch.max(torch.abs(W))\n",
                "    W_norm = W / abs_max\n",
                "    \n",
                "    # 2-bit levels\n",
                "    levels = torch.tensor([-1.0, -0.33, 0.33, 1.0])\n",
                "    \n",
                "    # Map each value to closest level\n",
                "    W_flat = W_norm.view(-1, 1)\n",
                "    diff = torch.abs(W_flat - levels.view(1, -1))\n",
                "    indices = torch.argmin(diff, dim=1)\n",
                "    \n",
                "    W_q = levels[indices].view(W.shape)\n",
                "    return W_q, abs_max\n",
                "\n",
                "# Demonstration\n",
                "W = torch.randn(1024, 1024)\n",
                "B, alpha = tpoti_1bit_quantize(W)\n",
                "W_target_2b, scale_2b = tpoti_2bit_quantize(W)\n",
                "\n",
                "print(f\"--- 1-bit Result ---\")\n",
                "print(f\"Binary Mask Sample: {B[0, :5].tolist()}\")\n",
                "print(f\"Optimal Alpha: {alpha:.4f}\")\n",
                "print(f\"1-bit MSE: {torch.mean((W - B*alpha)**2):.6f}\")\n",
                "\n",
                "print(f\"\\n--- 2-bit Result ---\")\n",
                "print(f\"Quaternary Sample: {W_target_2b[0, :5].tolist()}\")\n",
                "print(f\"2-bit MSE: {torch.mean((W - W_target_2b*scale_2b)**2):.6f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}