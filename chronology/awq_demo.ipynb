{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª AWQ From Scratch: Activation-aware Weight Quantization (2023)\n",
                "\n",
                "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/awq_demo.ipynb)\n",
                "\n",
                "## ðŸ“– The Theory: Protecting Salient Weights\n",
                "\n",
                "AWQ (Activation-aware Weight Quantization) is based on the observation that **not all weights are equally important**. Weights corresponding to large activation values (\"salient weights\") contribute significantly more to the final error if quantized poorly.\n",
                "\n",
                "### The Scaling Strategy\n",
                "Instead of searching for a complex non-linear mapping, AWQ simply **scales up** the most important weights before quantization. By multiplying a weight by $s > 1$, we move it to a higher precision region of the quantization grid. To maintain mathematical equivalence, we must scale down the activations by $1/s$.\n",
                "\n",
                "$$Y = (X \\cdot diag(1/s)) \\cdot (diag(s) \\cdot W)$$\n",
                "\n",
                "### Finding the Optimal Scale\n",
                "AWQ searches for a scale factor $s$ that minimizes the output error. A common heuristic is to use the activation magnitude raised to some power:\n",
                "\n",
                "$s = s_{X}^\\alpha$ where $s_X$ is the activation scale.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "def pseudo_quantize_tensor(w, n_bits, scale, zero):\n",
                "    \"\"\"Standard Min-Max Quantization Simulation\"\"\"\n",
                "    w_q = torch.round(w / scale + zero)\n",
                "    w_q = torch.clamp(w_q, 0, 2**n_bits - 1)\n",
                "    w_q = (w_q - zero) * scale\n",
                "    return w_q\n",
                "\n",
                "def awq_from_scratch(w, x, n_bits=4, n_grid=20):\n",
                "    \"\"\"\n",
                "    Simplified AWQ Logic.\n",
                "    w: [out_features, in_features] - Weight matrix\n",
                "    x: [batch, in_features] - Calibration activations\n",
                "    \"\"\"\n",
                "    # 1. Measure Activation Statistics (Scale of each input feature)\n",
                "    x_max = torch.mean(torch.abs(x), dim=0)\n",
                "    \n",
                "    # 2. Search for the best alpha (heuristic power for scaling)\n",
                "    best_error = float('inf')\n",
                "    best_s = None\n",
                "    \n",
                "    # Baseline weight stats (row-wise)\n",
                "    w_max = torch.max(torch.abs(w), dim=1, keepdim=True)[0]\n",
                "    \n",
                "    org_out = torch.matmul(x, w.t())\n",
                "    \n",
                "    print(\"Searching for optimal AWQ scaling factor...\")\n",
                "    for alpha in np.linspace(0, 1, n_grid):\n",
                "        # Scale based on activation magnitude\n",
                "        s = x_max.pow(alpha)\n",
                "        s = s / torch.sqrt(s.max() * s.min())  # Normalize scale\n",
                "        \n",
                "        # Apply scale to weight\n",
                "        w_scaled = w * s.view(1, -1)\n",
                "        \n",
                "        # Quantize the scaled weight\n",
                "        cur_max = torch.max(torch.abs(w_scaled), dim=1, keepdim=True)[0]\n",
                "        cur_scale = cur_max / (2**(n_bits-1) - 1)\n",
                "        w_q = torch.round(w_scaled / cur_scale) * cur_scale\n",
                "        \n",
                "        # Reverse scale for inference simulation\n",
                "        w_q_final = w_q / s.view(1, -1)\n",
                "        \n",
                "        # Measure error\n",
                "        cur_out = torch.matmul(x, w_q_final.t())\n",
                "        err = (org_out - cur_out).pow(2).mean()\n",
                "        \n",
                "        if err < best_error:\n",
                "            best_error = err\n",
                "            best_s = s\n",
                "            \n",
                "    print(f\"Best Error found: {best_error:.6f}\")\n",
                "    return best_s\n",
                "\n",
                "import numpy as np\n",
                "# Test implementation\n",
                "in_features, out_features = 512, 1024\n",
                "w = torch.randn(out_features, in_features)\n",
                "x = torch.randn(16, in_features)\n",
                "x[:, :10] *= 10.0  # Make some features salient\n",
                "\n",
                "s = awq_from_scratch(w, x)\n",
                "print(f\"Scale factor for salient feature 0: {s[0]:.4f}\")\n",
                "print(f\"Scale factor for normal feature 50: {s[50]:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}