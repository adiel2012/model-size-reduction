{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸª„ AWQ: Activation-aware Weight Quantization (2023)\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-quantization/blob/main/chronology/awq_demo.ipynb)\n",
                "\n",
                "AWQ identifies that not all weights are equally important. By looking at the activations, AWQ protects the most important weights (salient weights) from quantization error, leading to better accuracy than GPTQ in many cases, especially for smaller models.\n",
                "\n",
                "In this notebook, we use `AutoAWQ` to quantize an `OPT-125M` model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install autoawq transformers -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from awq import AutoAWQForCausalLM\n",
                "from transformers import AutoTokenizer\n",
                "import torch\n",
                "\n",
                "model_id = \"facebook/opt-125m\"\n",
                "quant_path = \"opt-125m-awq\"\n",
                "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
                "\n",
                "# 1. Load and Quantize\n",
                "print(\"--- Loading and Quantizing with AWQ ---\")\n",
                "model = AutoAWQForCausalLM.from_pretrained(model_id)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "\n",
                "model.quantize(tokenizer, quant_config=quant_config)\n",
                "\n",
                "# 2. Save (Optional for demo)\n",
                "# model.save_quantized(quant_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- AWQ Inference ---\")\n",
                "tokens = tokenizer(\"Deep learning quantization is\", return_tensors=\"pt\").to(\"cuda\")\n",
                "out = model.generate(**tokens, max_new_tokens=20)\n",
                "print(tokenizer.decode(out[0]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}