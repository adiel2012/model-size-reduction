{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ§ª AWQ From Scratch: Activation-aware Weight Quantization (2023)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/awq_demo.ipynb)\n\n## ðŸ“– Theory: Protecting Salient Weights\n\nAWQ (Lin et al., 2023) is a **calibration-light, hardware-friendly** quantization method.\nIts core insight: only ~1% of weight channels are *salient* -- yet quantizing them\npoorly accounts for the majority of the accuracy drop.\n\n### What Makes a Weight Salient?\n\nA weight $w_{ij}$ in a linear layer computes $y_j = \\sum_i x_i w_{ij}$.\nIf the corresponding activation $x_i$ is large, a small rounding error\n$\\delta w_{ij}$ causes a large output error $x_i \\cdot \\delta w_{ij}$.\n\n**Salient channels** = those with large expected activation magnitudes:\n\n$$s_i = \\mathbb{E}_X[|x_i|]$$\n\n### The Scaling Trick\n\nInstead of mixed-precision (hard for hardware), AWQ applies a **per-channel scale**\nbefore quantization and undoes it at inference time:\n\n$$Y = (X \\cdot \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s) \\cdot W)$$\n\nThe scaled weight $\\tilde{W} = \\text{diag}(s) \\cdot W$ is quantized to 4-bit.\nBecause the salient channel is scaled **up**, it gets finer quantization grid spacing\n-- more precision where the error matters most.\n\n### Finding the Optimal Scale\n\nThe scale is parameterised as $s = s_X^\\alpha$ where $s_X$ is the per-channel\nactivation magnitude. The power $\\alpha \\in [0,1]$ is found by **grid search**:\n\n$$\\alpha^* = \\arg\\min_\\alpha \\|\\tilde{W}_{\\alpha} X - W X\\|_2^2$$\n\n* $\\alpha = 0$: no scaling (pure weight quantization)\n* $\\alpha = 1$: full activation-domain scaling (like SmoothQuant)\n* AWQ typically finds $\\alpha \\approx 0.5$ optimal\n\n### AWQ vs SmoothQuant\n\n| | SmoothQuant | AWQ |\n|---|---|---|\n| Target | W8A8 (weights + activations INT8) | W4A16 (weights INT4, activations FP16) |\n| Scale absorbed into | Both $X$ and $W$ | Preceding layer norm (W only) |\n| Precision regime | 8-bit | 4-bit |\n\n### Limitations\n* Grid search over $\\alpha$ is heuristic; per-channel optimal scales not guaranteed.\n* Requires a small calibration set to estimate $s_X$.\n* Activation outliers are not clipped -- extreme activations still cause degradation.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def pseudo_quantize_tensor(w, n_bits, scale, zero):\n",
    "    \"\"\"Standard Min-Max Quantization Simulation\"\"\"\n",
    "    w_q = torch.round(w / scale + zero)\n",
    "    w_q = torch.clamp(w_q, 0, 2**n_bits - 1)\n",
    "    w_q = (w_q - zero) * scale\n",
    "    return w_q\n",
    "\n",
    "def awq_from_scratch(w, x, n_bits=4, n_grid=20):\n",
    "    \"\"\"\n",
    "    Simplified AWQ Logic.\n",
    "    w: [out_features, in_features] - Weight matrix\n",
    "    x: [batch, in_features] - Calibration activations\n",
    "    \"\"\"\n",
    "    # 1. Measure Activation Statistics (Scale of each input feature)\n",
    "    x_max = torch.mean(torch.abs(x), dim=0)\n",
    "    \n",
    "    # 2. Search for the best alpha (heuristic power for scaling)\n",
    "    best_error = float('inf')\n",
    "    best_s = None\n",
    "    \n",
    "    # Baseline weight stats (row-wise)\n",
    "    w_max = torch.max(torch.abs(w), dim=1, keepdim=True)[0]\n",
    "    \n",
    "    org_out = torch.matmul(x, w.t())\n",
    "    \n",
    "    print(\"Searching for optimal AWQ scaling factor...\")\n",
    "    for alpha in np.linspace(0, 1, n_grid):\n",
    "        # Scale based on activation magnitude\n",
    "        s = x_max.pow(alpha)\n",
    "        s = s / torch.sqrt(s.max() * s.min() + 1e-8)  # Normalize scale\n",
    "        \n",
    "        # Apply scale to weight\n",
    "        w_scaled = w * s.view(1, -1)\n",
    "        \n",
    "        # Quantize the scaled weight\n",
    "        cur_max = torch.max(torch.abs(w_scaled), dim=1, keepdim=True)[0]\n",
    "        cur_scale = (cur_max / (2**(n_bits-1) - 1)) + 1e-8\n",
    "        w_q = torch.round(w_scaled / cur_scale) * cur_scale\n",
    "        \n",
    "        # Reverse scale for inference simulation\n",
    "        w_q_final = w_q / s.view(1, -1)\n",
    "        \n",
    "        # Measure error\n",
    "        cur_out = torch.matmul(x, w_q_final.t())\n",
    "        err = (org_out - cur_out).pow(2).mean()\n",
    "        \n",
    "        if err < best_error:\n",
    "            best_error = err\n",
    "            best_s = s\n",
    "            \n",
    "    print(f\"Best Error found: {best_error:.6f}\")\n",
    "    return best_s\n",
    "\n",
    "# Test implementation\n",
    "in_features, out_features = 512, 1024\n",
    "w = torch.randn(out_features, in_features)\n",
    "x = torch.randn(16, in_features)\n",
    "x[:, :10] *= 10.0  # Make some features salient\n",
    "\n",
    "s = awq_from_scratch(w, x)\n",
    "print(f\"Scale factor for salient feature 0: {s[0]:.4f}\")\n",
    "print(f\"Scale factor for normal feature 50: {s[50]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: AWQ protects a salient input channel\nimport torch\n\nprint(\"=== AWQ: scaling a salient channel before quantization ===\\n\")\n\n# 2 outputs, 4 inputs â€” channel 2 is salient\nW     = torch.tensor([[ 0.5, -0.3,  0.8, -0.2],\n                       [-0.4,  0.7, -0.6,  0.3]])\nx_max = torch.tensor([0.10, 0.15, 8.00, 0.12])   # activation scales\n\nprint(f\"Weight matrix W (2Ã—4):\\n{W.numpy()}\")\nprint(f\"\\nActivation magnitude per input channel: {x_max.tolist()}\")\nprint(f\"  â†’ Channel 2 is SALIENT (8.0 vs ~0.1 for the others)\")\n\ndef quant4(w):\n    \"\"\"Row-wise 4-bit symmetric quantization.\"\"\"\n    q_max = 7\n    scale = w.abs().max(dim=1, keepdim=True)[0] / q_max + 1e-8\n    return (w / scale).round().clamp(-q_max, q_max) * scale\n\n# Baseline â€” naive 4-bit, no scaling\nW_q_naive = quant4(W)\nmse_naive  = (W - W_q_naive).pow(2).mean()\nprint(f\"\\n--- Naive 4-bit (no AWQ) ---\")\nprint(f\"Quantized W:\\n{W_q_naive.numpy().round(4)}\")\nprint(f\"MSE: {mse_naive:.6f}\")\n\n# AWQ â€” scale salient channel UP before quantizing\nalpha = 0.5\ns     = x_max.pow(alpha)\ns     = s / (s.max() * s.min()).sqrt()       # normalize scale\n\nprint(f\"\\n--- AWQ (alpha={alpha}) ---\")\nprint(f\"Per-channel scale s = x_max^0.5 (normalized): {s.tolist()}\")\nprint(f\"  â†’ Channel 2 scaled by {s[2]:.2f}Ã— before quantization (more precision allocated)\")\n\nW_scaled  = W * s.view(1, -1)              # scale up salient weights\nW_q_scaled = quant4(W_scaled)\nW_q_awq   = W_q_scaled / s.view(1, -1)    # undo scale for inference\nmse_awq   = (W - W_q_awq).pow(2).mean()\n\nprint(f\"Quantized W (after unscaling):\\n{W_q_awq.numpy().round(4)}\")\nprint(f\"MSE: {mse_awq:.6f}\")\nprint(f\"\\nAWQ improvement: {(1 - mse_awq/mse_naive)*100:.1f}% lower MSE for the same 4-bit budget\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\n# Collect per-layer input activation stats via hooks\nact_stats = {}\ndef make_stat_hook(name):\n    def hook(module, inp, out):\n        x = inp[0].detach().float().reshape(-1, inp[0].shape[-1])\n        act_stats[name] = torch.mean(torch.abs(x), dim=0)\n    return hook\n\nhooks = []\nfor name, module in model.named_modules():\n    if hasattr(module, \"weight\") and module.weight is not None and module.weight.dim() == 2:\n        hooks.append(module.register_forward_hook(make_stat_hook(name)))\nwith torch.no_grad():\n    model(**inputs)\nfor h in hooks:\n    h.remove()\n\n# Apply AWQ (alpha=0.5) + 4-bit quantization to all 2D weights\nn_bits, q_max = 4, 7\nmodel_q = copy.deepcopy(model)\nfor name, module in model_q.named_modules():\n    if hasattr(module, \"weight\") and module.weight is not None and module.weight.dim() == 2:\n        w = module.weight.data.float()\n        x_max = act_stats.get(name)\n        if x_max is not None and x_max.shape[0] == w.shape[1]:\n            s = x_max.pow(0.5)\n            s = s / (s.max() * s.min()).sqrt().clamp(min=1e-8)\n            w_scaled = w * s.view(1, -1)\n        else:\n            w_scaled, s = w, None\n        scale = torch.max(torch.abs(w_scaled)) / q_max + 1e-8\n        w_q = torch.round(w_scaled / scale).clamp(-q_max, q_max) * scale\n        if s is not None:\n            w_q = w_q / s.view(1, -1)\n        module.weight.data = w_q.to(module.weight.dtype)\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"AWQ GPT-2 Perplexity:      {quant_ppl:.2f}\")\nprint(f\"Delta:                     {quant_ppl - baseline_ppl:+.2f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ“š References\n\n1. **Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., & Han, S.** (2023).  \n   *AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration.* MLSys 2024.  \n   [arXiv:2306.00978](https://arxiv.org/abs/2306.00978)\n\n2. **Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., & Han, S.** (2022).  \n   *SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.* ICML 2023.  \n   [arXiv:2211.10438](https://arxiv.org/abs/2211.10438)\n\n3. **Frantar, E., Ashkboos, S., Hoefler, T., & Alistarh, D.** (2022).  \n   *GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.* ICLR 2023.  \n   [arXiv:2210.17323](https://arxiv.org/abs/2210.17323)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}