{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª AWQ From Scratch: Activation-aware Weight Quantization (2023)\n",
    "\n",
    "[![\"Open In Colab\"](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-size-reduction/blob/main/chronology/awq_demo.ipynb)\n",
    "\n",
    "## ðŸ“– The Theory: Protecting Salient Weights\n",
    "\n",
    "AWQ (Activation-aware Weight Quantization) is based on the observation that **not all weights are equally important**. Weights corresponding to large activation values (\"salient weights\") contribute significantly more to the final error if quantized poorly.\n",
    "\n",
    "### The Scaling Strategy\n",
    "Instead of searching for a complex non-linear mapping, AWQ simply **scales up** the most important weights before quantization. By multiplying a weight by $s > 1$, we move it to a higher precision region of the quantization grid. To maintain mathematical equivalence, we must scale down the activations by $1/s$.\n",
    "\n",
    "$$Y = (X \\cdot diag(1/s)) \\cdot (diag(s) \\cdot W)$$\n",
    "\n",
    "### Finding the Optimal Scale\n",
    "AWQ searches for a scale factor $s$ that minimizes the output error. A common heuristic is to use the activation magnitude raised to some power:\n",
    "\n",
    "$s = s_{X}^\\alpha$ where $s_X$ is the activation scale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def pseudo_quantize_tensor(w, n_bits, scale, zero):\n",
    "    \"\"\"Standard Min-Max Quantization Simulation\"\"\"\n",
    "    w_q = torch.round(w / scale + zero)\n",
    "    w_q = torch.clamp(w_q, 0, 2**n_bits - 1)\n",
    "    w_q = (w_q - zero) * scale\n",
    "    return w_q\n",
    "\n",
    "def awq_from_scratch(w, x, n_bits=4, n_grid=20):\n",
    "    \"\"\"\n",
    "    Simplified AWQ Logic.\n",
    "    w: [out_features, in_features] - Weight matrix\n",
    "    x: [batch, in_features] - Calibration activations\n",
    "    \"\"\"\n",
    "    # 1. Measure Activation Statistics (Scale of each input feature)\n",
    "    x_max = torch.mean(torch.abs(x), dim=0)\n",
    "    \n",
    "    # 2. Search for the best alpha (heuristic power for scaling)\n",
    "    best_error = float('inf')\n",
    "    best_s = None\n",
    "    \n",
    "    # Baseline weight stats (row-wise)\n",
    "    w_max = torch.max(torch.abs(w), dim=1, keepdim=True)[0]\n",
    "    \n",
    "    org_out = torch.matmul(x, w.t())\n",
    "    \n",
    "    print(\"Searching for optimal AWQ scaling factor...\")\n",
    "    for alpha in np.linspace(0, 1, n_grid):\n",
    "        # Scale based on activation magnitude\n",
    "        s = x_max.pow(alpha)\n",
    "        s = s / torch.sqrt(s.max() * s.min() + 1e-8)  # Normalize scale\n",
    "        \n",
    "        # Apply scale to weight\n",
    "        w_scaled = w * s.view(1, -1)\n",
    "        \n",
    "        # Quantize the scaled weight\n",
    "        cur_max = torch.max(torch.abs(w_scaled), dim=1, keepdim=True)[0]\n",
    "        cur_scale = (cur_max / (2**(n_bits-1) - 1)) + 1e-8\n",
    "        w_q = torch.round(w_scaled / cur_scale) * cur_scale\n",
    "        \n",
    "        # Reverse scale for inference simulation\n",
    "        w_q_final = w_q / s.view(1, -1)\n",
    "        \n",
    "        # Measure error\n",
    "        cur_out = torch.matmul(x, w_q_final.t())\n",
    "        err = (org_out - cur_out).pow(2).mean()\n",
    "        \n",
    "        if err < best_error:\n",
    "            best_error = err\n",
    "            best_s = s\n",
    "            \n",
    "    print(f\"Best Error found: {best_error:.6f}\")\n",
    "    return best_s\n",
    "\n",
    "# Test implementation\n",
    "in_features, out_features = 512, 1024\n",
    "w = torch.randn(out_features, in_features)\n",
    "x = torch.randn(16, in_features)\n",
    "x[:, :10] *= 10.0  # Make some features salient\n",
    "\n",
    "s = awq_from_scratch(w, x)\n",
    "print(f\"Scale factor for salient feature 0: {s[0]:.4f}\")\n",
    "print(f\"Scale factor for normal feature 50: {s[50]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ”¢ Worked Example with Numbers\n\nBefore the full implementation, letâ€™s trace through the math with a tiny, hand-traceable example."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tiny example: AWQ protects a salient input channel\nimport torch\n\nprint(\"=== AWQ: scaling a salient channel before quantization ===\\n\")\n\n# 2 outputs, 4 inputs â€” channel 2 is salient\nW     = torch.tensor([[ 0.5, -0.3,  0.8, -0.2],\n                       [-0.4,  0.7, -0.6,  0.3]])\nx_max = torch.tensor([0.10, 0.15, 8.00, 0.12])   # activation scales\n\nprint(f\"Weight matrix W (2Ã—4):\\n{W.numpy()}\")\nprint(f\"\\nActivation magnitude per input channel: {x_max.tolist()}\")\nprint(f\"  â†’ Channel 2 is SALIENT (8.0 vs ~0.1 for the others)\")\n\ndef quant4(w):\n    \"\"\"Row-wise 4-bit symmetric quantization.\"\"\"\n    q_max = 7\n    scale = w.abs().max(dim=1, keepdim=True)[0] / q_max + 1e-8\n    return (w / scale).round().clamp(-q_max, q_max) * scale\n\n# Baseline â€” naive 4-bit, no scaling\nW_q_naive = quant4(W)\nmse_naive  = (W - W_q_naive).pow(2).mean()\nprint(f\"\\n--- Naive 4-bit (no AWQ) ---\")\nprint(f\"Quantized W:\\n{W_q_naive.numpy().round(4)}\")\nprint(f\"MSE: {mse_naive:.6f}\")\n\n# AWQ â€” scale salient channel UP before quantizing\nalpha = 0.5\ns     = x_max.pow(alpha)\ns     = s / (s.max() * s.min()).sqrt()       # normalize scale\n\nprint(f\"\\n--- AWQ (alpha={alpha}) ---\")\nprint(f\"Per-channel scale s = x_max^0.5 (normalized): {s.tolist()}\")\nprint(f\"  â†’ Channel 2 scaled by {s[2]:.2f}Ã— before quantization (more precision allocated)\")\n\nW_scaled  = W * s.view(1, -1)              # scale up salient weights\nW_q_scaled = quant4(W_scaled)\nW_q_awq   = W_q_scaled / s.view(1, -1)    # undo scale for inference\nmse_awq   = (W - W_q_awq).pow(2).mean()\n\nprint(f\"Quantized W (after unscaling):\\n{W_q_awq.numpy().round(4)}\")\nprint(f\"MSE: {mse_awq:.6f}\")\nprint(f\"\\nAWQ improvement: {(1 - mse_awq/mse_naive)*100:.1f}% lower MSE for the same 4-bit budget\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ§ª GPT-2 Evaluation\n\nApply the method to all 2D weight matrices of GPT-2 and compare perplexity before and after quantization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch, copy\n\nmodel_id = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_id)\nmodel = GPT2LMHeadModel.from_pretrained(model_id).eval()\n\ntext = \"The quick brown fox jumps over the lazy dog. Transformers are powerful sequence models.\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\ndef perplexity(mdl, inputs):\n    with torch.no_grad():\n        loss = mdl(**inputs, labels=inputs[\"input_ids\"]).loss\n    return torch.exp(loss).item()\n\nbaseline_ppl = perplexity(model, inputs)\nprint(f\"Baseline GPT-2 Perplexity: {baseline_ppl:.2f}\")\n\n# Collect per-layer input activation stats via hooks\nact_stats = {}\ndef make_stat_hook(name):\n    def hook(module, inp, out):\n        x = inp[0].detach().float().reshape(-1, inp[0].shape[-1])\n        act_stats[name] = torch.mean(torch.abs(x), dim=0)\n    return hook\n\nhooks = []\nfor name, module in model.named_modules():\n    if hasattr(module, \"weight\") and module.weight is not None and module.weight.dim() == 2:\n        hooks.append(module.register_forward_hook(make_stat_hook(name)))\nwith torch.no_grad():\n    model(**inputs)\nfor h in hooks:\n    h.remove()\n\n# Apply AWQ (alpha=0.5) + 4-bit quantization to all 2D weights\nn_bits, q_max = 4, 7\nmodel_q = copy.deepcopy(model)\nfor name, module in model_q.named_modules():\n    if hasattr(module, \"weight\") and module.weight is not None and module.weight.dim() == 2:\n        w = module.weight.data.float()\n        x_max = act_stats.get(name)\n        if x_max is not None and x_max.shape[0] == w.shape[1]:\n            s = x_max.pow(0.5)\n            s = s / (s.max() * s.min()).sqrt().clamp(min=1e-8)\n            w_scaled = w * s.view(1, -1)\n        else:\n            w_scaled, s = w, None\n        scale = torch.max(torch.abs(w_scaled)) / q_max + 1e-8\n        w_q = torch.round(w_scaled / scale).clamp(-q_max, q_max) * scale\n        if s is not None:\n            w_q = w_q / s.view(1, -1)\n        module.weight.data = w_q.to(module.weight.dtype)\n\nquant_ppl = perplexity(model_q, inputs)\nprint(f\"AWQ GPT-2 Perplexity:      {quant_ppl:.2f}\")\nprint(f\"Delta:                     {quant_ppl - baseline_ppl:+.2f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}