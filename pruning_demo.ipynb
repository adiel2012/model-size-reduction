{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚úÇÔ∏è Model Pruning: Removing Unnecessary Weights\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/model-quantization/blob/main/pruning_demo.ipynb)\n",
                "\n",
                "Model Pruning is a compression technique that removes redundant parameters from a neural network. This is typically done by setting small weights to zero. \n",
                "\n",
                "### Types of Pruning:\n",
                "1. **Unstructured Pruning**: Individual weights are removed. This leads to sparse matrices but requires specialized hardware/software for real speedup.\n",
                "2. **Structured Pruning**: Entire neurons, channels, or layers are removed. This leads to smaller dense matrices that are easier to accelerate on standard hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.utils.prune as prune\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
                "\n",
                "# 1. Load GPT-2\n",
                "model_id = \"gpt2\"\n",
                "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
                "\n",
                "print(f\"Initial Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üî™ Global Unstructured Pruning\n",
                "We will prune 30% of the weights in all linear layers using L1-norm magnitude (removing the smallest weights)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "parameters_to_prune = []\n",
                "for name, module in model.named_modules():\n",
                "    if isinstance(module, torch.nn.Linear):\n",
                "        parameters_to_prune.append((module, 'weight'))\n",
                "\n",
                "prune.global_unstructured(\n",
                "    parameters_to_prune,\n",
                "    pruning_method=prune.L1Unstructured,\n",
                "    amount=0.3, # Prune 30%\n",
                ")\n",
                "\n",
                "print(\"Global pruning (30%) applied.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Verifying Sparsity\n",
                "Let's check how many weights are now exactly zero."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_sparsity(model):\n",
                "    total_zeros = 0\n",
                "    total_elements = 0\n",
                "    for name, module in model.named_modules():\n",
                "        if isinstance(module, torch.nn.Linear):\n",
                "            total_zeros += torch.sum(module.weight == 0).item()\n",
                "            total_elements += module.weight.numel()\n",
                "    \n",
                "    sparsity = 100. * total_zeros / total_elements\n",
                "    print(f\"Global Sparsity: {sparsity:.2f}%\")\n",
                "    return sparsity\n",
                "\n",
                "calculate_sparsity(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèéÔ∏è Generation Test\n",
                "Does the model still generate coherent text after losing 30% of its connections?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_text = \"Neural network pruning is used to\"\n",
                "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    output = model.generate(**inputs, max_length=30, do_sample=True)\n",
                "\n",
                "print(tokenizer.decode(output[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}